{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to LiGHT!","text":"<p>LiGHT stands for Laboratory for Intelligent Global Health and Humanitarian Response Technologies</p> <p>If you are new to LiGHT, start by reading the Getting Started tutorial</p>"},{"location":"about/","title":"About LiGHT","text":"<p>We are LiGHT (yes)</p>"},{"location":"clusters/","title":"Connecting to the clusters","text":"<p>Throughout your work at LiGHT, you will need to run some heavy computations (whether for inference or training). LiGHT has access to many different services to run those computations</p> <ul> <li>EPFL RCP: a cluster hosted at EPFL. This cluster should be used for inference, small training and experiments on small models</li> <li>SwissAI CSCS: a cluster maded by SwissAI. This cluster should be used for larger training on bigger models.</li> </ul>"},{"location":"contributing/","title":"How to contribute to LiGHT docs","text":""},{"location":"contributing/#guidelines","title":"Guidelines","text":"<p>To contribute, you can suggest changes and open pull requests at our GitHub repo</p> <p><code>LiGHT-doc</code> uses <code>mkdocs</code>. <code>mkdocs</code> is a tool that automatically build documentation from markdown files.</p> <p>For full documentation visit mkdocs.org.</p>"},{"location":"contributing/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> <p>Every markdown files are written in the <code>docs</code> directory. </p>"},{"location":"contributing/#workflow","title":"Workflow","text":"<ul> <li> <p>Update your relevant files in the <code>docs</code> folder</p> </li> <li> <p>Test your local changes by running <pre><code>mkdocs serve\n</code></pre> and accessing <code>localhost:8000</code> on your browser</p> </li> <li> <p>Push your changes to some branch <pre><code>git checkout -b [branch name]\ngit add [files]\ngit commit -m [Commit message]\ngit push\n</code></pre></p> </li> <li> <p>Open a pull request to the <code>main</code> branch on GitHub</p> </li> <li> <p>Once the pull request is merged on <code>main</code>, changes should be visible at https://light-yalepfl.github.io/LiGHT-doc/</p> </li> </ul>"},{"location":"gettingstarted/","title":"Getting started","text":"<p>Welcome to LiGHT! </p> <p>For your projects at LiGHT, you will probably need GPUs (for model inference or training). To get access to those resources, check the dedicated section</p>"},{"location":"projects/","title":"Projects for Fall 2025","text":""},{"location":"projects/#1-meditron","title":"1. Meditron","text":"<ul> <li>MultiMeditron</li> </ul> <p>This project is about making Meditron multimodal: the user can provide Meditron with medical images, in addition to text. Work is two-fold: adapting the codebase of Meditron to make it have a multimodal architecture, and making the \"expert\" models that process the images and make embeddings fed to Meditron.</p> <p>Contact: Michael Zhang (michael.zhang@epfl.ch)</p> <ul> <li>Fine-tuning multimodal models for the medical use</li> </ul> <p>This project aims to fine-tune generalist SOTA multimodal models (Qwen3 Omni, Llava, Llama4,...) with our medical multimodal data mixture. The goal is to build the best open-weights medical multimodal model according to the standard benchmark</p> <p>Contact: Michael Zhang (michael.zhang@epfl.ch)</p> <ul> <li>Meditron Reasoning</li> </ul> <p>This project aims to improve our training pipeline by integrating novel reinforcement learning approaches, notably using GRPO algorithms. This is the continuation of a previous project conducted in this area, and we plan to expand the existing work to enhance our project performances (add MultiMeditron for multi-modal reasoning).</p> <p>Contact: Guillaume Boy\u00e9 (guillaume.boye@epfl.ch)</p> <ul> <li>Polyglot Meditron &amp; Giving Meditron a Voice</li> </ul> <p>Speaking English is nice, most content online is in English. Having a performant LLM for medical tasks formulated in English is useful. But not enough! In low-resource settings and even in most places of the globe, people usually prefer using their first language rather than English.</p> <p>There are many people around the world who even though cannot read, seek healthcare information and guidance. Currently, medical LLMs, even those that are multi-modal, are usually constrained to a few languages thereby limiting their application in this particular use-case of healthcare question answering. The main objective of this project is to extend the multi-lingual speech capabilities of our Meditron model to ensure that it is more accessible to people around the world.</p> <p>This project aims at making Meditron models more proficient in other languages, with a focus on low-resource languages. In written and spoken speech. Work is needed, since having a polyglot base model is generally not enough: popular models do not have a focus on low-resource languages, and there is also a need to make sure to teach the model non-English medical terminology.</p> <p>Contact: Fabrice Nemo (fabrice.nemo@epfl.ch) &amp; David Sasu (david.sasu@epfl.ch)</p> <ul> <li>NeuroMeditron</li> </ul> <p>NeuroMeditron develops robust multimodal models for dementia prediction using voice and typing dynamics from the mPower dataset. The project focuses on handling missing modalities through advanced fusion strategies, enabling reliable patient-level monitoring. A proof-of-concept \u201cNeuro Expert\u201d adapter will integrate these digital biomarkers into MultiMeditron.</p> <p>Contact: Arianna Francesconi (arianna.francesconi@epfl.ch)</p>"},{"location":"projects/#2-mmore","title":"2. MMORE","text":"<p>MMORE stands for Massive Multimodal Open RAG &amp; Extraction, it is our Python library for a scalable multimodal pipeline for processing, indexing, and querying multimodal documents.</p> <p>GitHub repo</p> <p>Contact: Fabrice Nemo (fabrice.nemo@epfl.ch)</p>"},{"location":"projects/#3-moove","title":"3. Moove","text":"<p>The moove is a collaborative platform where experts and communities co-design and validate AI models. The initiative focuses on aligning large language models with real-world standards, ensuring they are transparent, safe, and context-aware. It is already partnered with institutions such as CHUV, ICRC, the Gates Foundation and many hospitals around the world.</p> <p>If you want to help us make the moove even greater, don't hesitate to join!</p> <p>Note that the project is software-engineering focused.</p> <p>Contact: Bryan Gotti (bryan.gotti@epfl.ch)</p>"},{"location":"projects/#4-hic-lab-ai-bootcamp","title":"4. HIC-Lab AI Bootcamp","text":"<p>Very cool project about teaching the basics of AI applied to healthcare. The target audience is healthcare workers and computer scientists in Rwanda. Our work in LiGHT is to improve the content of the bootcamp so that students learn better, and mentor students there, guide them throughout their completion of the bootcamp.</p> <p>Contact: Fabrice Nemo (fabrice.nemo@epfl.ch)</p>"},{"location":"projects/#5-chit-chat","title":"5. CHIT-CHAT","text":"<ol> <li>Embedding Humanitarian Principles in LLM Development</li> </ol> <p>LLMs are usually not deployed for humanitarian applications since they are not intentionally designed to align to humanitarian values. This project therefore aims to develop a framework / checklist for LLM development and evaluation that can be applied in the creation and testing of Humanitarian-focused LLMs.</p> <p>Contact: David Sasu (david.sasu@epfl.ch)</p>"},{"location":"projects/#6-prism-ai","title":"6. PRISM-AI","text":"<p>PRISM-AI leverages the PRISM dataset on pregnancy reference intervals to benchmark traditional ML/DL models against Large Language Models for risk prediction in maternal health. The project explores fine-tuning strategies and novel optimization methods (e.g., DPO/GRPO) to assess whether LLMs can provide clinically meaningful improvements over established approaches.</p> <p>Contact: Arianna Francesconi (arianna.francesconi@epfl.ch)</p>"},{"location":"projects/#7-multimodal-learning-from-voice-and-keyboard-dynamics-for-early-alzheimers-diagnosis","title":"7. Multimodal Learning from Voice and Keyboard Dynamics for Early Alzheimer\u2019s Diagnosis","text":"<p>This project develops deep learning model to detect early Alzheimer\u2019s disease from typing and voice signals. Students will design a multimodal models (RNNs for typing and CNN/ViT for voice) to capture motor and speech patterns linked to cognitive decline, comparing modality contributions and model interpretability.</p> <p>Contact: Arianna Francesconi (arianna.francesconi@epfl.ch)</p>"},{"location":"projects/#8-cross-disease-voice-prognosis-parkinson-and-als-audio-modeling","title":"8. Cross-Disease Voice Prognosis: Parkinson and ALS Audio Modeling","text":"<p>Voice changes are early markers of neurodegenerative diseases. This project trains deep learning models on Parkinson\u2019s voice recordings (mPower) and tests cross-disease generalization on ALS speech data, exploring transfer learning and shared vocal biomarkers across disorders.</p> <p>Contact: Arianna Francesconi (arianna.francesconi@epfl.ch)</p>"},{"location":"projects/#9-balancing-time-series-health-data-across-diseases","title":"9. Balancing Time-Series Health Data Across Diseases","text":"<p>This project extends the IMBALMED method for class balancing in time-series models (LSTM/GRU) and benchmarks it against standard techniques such as SMOTE or focal loss. Students will analyze cross-disease robustness and ensemble diversity, building a reproducible benchmark for temporal health data.</p> <p>Contact: Arianna Francesconi (arianna.francesconi@epfl.ch)</p>"},{"location":"clusters/cscs/","title":"Connecting to CSCS","text":"<p>DO NOT RUN ON LOGIN NODE</p> <p>When you establish a direct connection using <code>ssh</code> you connect to the login node. Everyone is on that node and as such YOU SHOULD NEVER RUN ANY JOBS DIRECTLY ON THE LOGIN NODE. If you want to run a process, like a training, you can run it on a dedicated allocated job</p>"},{"location":"clusters/cscs/#pre-setup-access-to-the-cscs","title":"Pre-setup (access to the CSCS)","text":"<p>Please ask Michael or Peter to add you to the CSCS project (send a message on Slack to get a faster answer). Once you have been added, check your mail for the invitation link. You will to have to create an account.</p>"},{"location":"clusters/cscs/#connect-to-the-login-node","title":"Connect to the login node","text":"<p>To connect to the login node, you will need to refresh your key every 24 hours. To refresh your keys, you need to execute the following script. Store the following script in a <code>.sh</code> file (e.g. <code>cscs_connect.sh</code>). Make sure to replace <code>$CSCS_USERNAME</code> with your CSCS username and the <code>$CSCS_PASSWORD</code> with your CSCS password.</p> <pre><code>#!/bin/bash\n\n# This script sets the environment properly so that a user can access CSCS\n# login nodes via ssh. \n\n#    Copyright (C) 2023, ETH Zuerich, Switzerland\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, version 3 of the License.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.\n#\n#    AUTHORS Massimo Benini\n\n\nUSERNAME=$CSCS_USERNAME\nPASSWORD=$CSCS_PASSWORD\n#read -p \"Username : \" USERNAME\n#read -s -p \"Password: \" PASSWORD\n\nfunction ProgressBar {\n# Process data\n    let _progress=(${1}*100/${2}*100)/100\n    let _done=(${_progress}*4)/10\n    let _left=40-$_done\n# Build progressbar string lengths\n    _fill=$(printf \"%${_done}s\")\n    _empty=$(printf \"%${_left}s\")\n\n# 1.2 Build progressbar strings and print the ProgressBar line\n# 1.2.1 Output example:\n# 1.2.1.1 Progress : [########################################] 100%\nprintf \"\\rSetting the environment : [${_fill// /#}${_empty// /-}] ${_progress}%%\"\n}\n\n#Variables\n_start=1\n#This accounts as the \"totalState\" variable for the ProgressBar function\n_end=100\n\n#Params\nMFA_KEYS_URL=\"https://sshservice.cscs.ch/api/v1/auth/ssh-keys/signed-key\"\n\n#Detect OS\nOS=\"$(uname)\"\ncase \"${OS}\" in\n  'Linux')\n    OS='Linux'\n    ;;\n  'FreeBSD')\n    OS='FreeBSD'\n    ;;\n  'WindowsNT')\n    OS='Windows'\n    ;;\n  'Darwin')\n    OS='Mac'\n    ;;\n  *) ;;\nesac\n\n#OS validation\nif [ \"${OS}\" != \"Mac\" ] &amp;&amp; [ \"${OS}\" != \"Linux\" ]; then\n  echo \"This script works only on Mac-OS or Linux. Abording.\"\n  exit 1\nfi\n\n#Read Inputs\necho\nread -s -p \"Enter OTP (6-digit code): \" OTP\necho\n\nif [ -z \"${PASSWORD}\" ]; then\n    echo \"Password is empty.\"\n    exit 1\nfi\n\nif ! [[ \"${OTP}\" =~ ^[[:digit:]]{6} ]]; then\n    echo \"OTP is not valid, OTP must contains only six digits.\"\n    exit 1\nfi\n\nProgressBar 25 \"${_end}\"\necho \"  Authenticating to the SSH key service...\"\n\nHEADERS=(-H \"Content-Type: application/json\" -H \"accept: application/json\")\nKEYS=$(curl -s -S --ssl-reqd \\\n    \"${HEADERS[@]}\" \\\n    -d \"{\\\"username\\\": \\\"$USERNAME\\\", \\\"password\\\": \\\"$PASSWORD\\\", \\\"otp\\\": \\\"$OTP\\\"}\" \\\n    \"$MFA_KEYS_URL\")\n\nif [ $? != 0 ]; then\n    exit 1\nfi\n\nProgressBar 50 \"${_end}\"\necho \"  Retrieving the SSH keys...\"\n\nDICT_KEY=$(echo ${KEYS} | cut -d \\\" -f 2)\nif [ \"${DICT_KEY}\" == \"payload\" ]; then\n   MESSAGE=$(echo ${KEYS} | cut -d \\\" -f 6)\n   ! [ -z \"${MESSAGE}\" ] &amp;&amp; echo \"${MESSAGE}\"\n   echo \"Error fetching the SSH keys. Aborting.\"\n   exit 1\nfi\n\nPUBLIC=$(echo ${KEYS} | cut -d \\\" -f 4)\nPRIVATE=$(echo ${KEYS} | cut -d \\\" -f 8)\n\n#Check if keys are empty:\nif [ -z \"${PUBLIC}\" ] || [ -z \"${PRIVATE}\" ]; then\n    echo \"Error fetching the SSH keys. Aborting.\"\n    exit 1\nfi\n\nProgressBar 75 \"${_end}\"\necho \"  Setting up the SSH keys into your home folder...\"\n\n#Check ~/.ssh folder and store the keys\necho ${PUBLIC} | awk '{gsub(/\\\\n/,\"\\n\")}1' &gt; ~/.ssh/cscs-key-cert.pub || exit 1\necho ${PRIVATE} | awk '{gsub(/\\\\n/,\"\\n\")}1' &gt; ~/.ssh/cscs-key || exit 1\n\n#Setting permissions:\nchmod 644 ~/.ssh/cscs-key-cert.pub || exit 1\nchmod 600 ~/.ssh/cscs-key || exit 1\n\n#Format the keys:\nif [ \"${OS}\" = \"Mac\" ]\nthen\n  sed -i '' -e '$ d' ~/.ssh/cscs-key-cert.pub || exit 1\n  sed -i '' -e '$ d' ~/.ssh/cscs-key || exit 1\nelse [ \"${OS}\" = \"Linux\" ]\n  sed '$d' ~/.ssh/cscs-key-cert.pub || exit 1\n  sed '$d' ~/.ssh/cscs-key || exit 1\nfi\n\nProgressBar 100 \"${_end}\"\necho \"  Completed.\"\n\nexit_code_passphrase=1\nread -n 1 -p \"Do you want to add a passphrase to your key? [y/n] (Default y) \" reply; \nif [ \"$reply\" != \"\" ];\n then echo;\nfi\nif [ \"$reply\" = \"${reply#[Nn]}\" ]; then\n      while [ $exit_code_passphrase != 0 ]; do\n        ssh-keygen -f ~/.ssh/cscs-key -p\n        exit_code_passphrase=$?\n      done\nfi\n\nif (( $exit_code_passphrase == 0 ));\n  then\n    SUBSTRING=\", using the passphrase you have set:\";\n  else\n     SUBSTRING=\":\";\nfi     \n\neval `ssh-agent -s`\nssh-add -t 1d ~/.ssh/cscs-key\n</code></pre> <p>You will have to execute this bash script every day. If you don't want to have your login ID stored in a script, you can comment out the lines:</p> <pre><code>#read -p \"Username : \" USERNAME\n#read -s -p \"Password: \" PASSWORD\n</code></pre> <p>and remove the lines:</p> <pre><code>USERNAME=$CSCS_USERNAME\nPASSWORD=$CSCS_PASSWORD\n</code></pre>"},{"location":"clusters/cscs/#setup-your-ssh-config","title":"Setup your ssh config","text":"<p>Add the following lines to the <code>~/.ssh/config</code> file:</p> <pre><code>Host ela\n    HostName ela.cscs.ch\n    User $CSCS_USERNAME\n    ForwardAgent yes\n    ForwardX11 yes\n    forwardX11Trusted yes\n    IdentityFile ~/.ssh/cscs-key\n\n\nHost todi\n    HostName todi.cscs.ch\n    User $CSCS_USERNAME\n    ProxyJump ela\n    ForwardAgent yes\n    ForwardX11 yes\n    forwardX11Trusted yes\n    IdentityFile ~/.ssh/cscs-key\n\nHost clariden\n    HostName clariden.cscs.ch\n    User $CSCS_USERNAME\n    ProxyJump ela\n    ForwardAgent yes\n    ForwardX11 yes\n    forwardX11Trusted yes\n    IdentityFile ~/.ssh/cscs-key\n</code></pre> <p>To connect to the cluster, run the following:</p> <pre><code># Your terminal\n\nssh clariden\n</code></pre> <p>DO NOT RUN ON LOGIN NODE</p> <p>This opens a terminal on the CSCS login node. You should have a terminal that looks like this: <pre><code>[mzhang@clariden-lnxxx ~]$\n</code></pre></p> <p>DO NOT RUN ANYTHING HEAVY WHEN YOU SEE THIS PROMPT (training, inference, heavy download, etc.). You should always ask for a job first. Here <code>ln</code> stands for login node, this is how you know that you are on the login node. To launch a job, see how to use an environment and how to launch a job</p>"},{"location":"clusters/cscs/#setup-github","title":"Setup Github","text":"<p>To operate on private repositories on GitHub. You can either generate a SSH key pairs or use a GitHub personal access token (GitHub PAT). We recommand doing the second option but both options are viable.</p> <p>To generate a GitHub PAT, follow those instructions. Make sure that this PAT is stored somewhere</p> <p>For this tutorial, we are gonna use the <code>MultiMeditron</code> training pipeline setup. Clone the MultiMeditron repository in your user directory:</p> HTTP cloneSSH clone <pre><code># CSCS login node\n\nmkdir /users/$CSCS_USERNAME/meditron\ncd /users/$CSCS_USERNAME/meditron\n\ngit clone https://github.com/EPFLiGHT/MultiMeditron.git\n</code></pre> <pre><code># CSCS login node\n\nmkdir /users/$CSCS_USERNAME/meditron\ncd /users/$CSCS_USERNAME/meditron\n\ngit clone git@github.com:EPFLiGHT/MultiMeditron.git\n</code></pre> <p>When GitHub asks for your password, input the PAT that you have generated in this step.</p>"},{"location":"clusters/cscs/#create-a-personal-folder-in-the-capstor-partition","title":"Create a personal folder in the capstor partition","text":"<pre><code># CSCS login node\n\nmkdir /capstor/store/cscs/swissai/a127/homes/$CSCS_USERNAME\n</code></pre> <p>This personal folder on the capstor will be mainly used to store your huggingface home and your big files that don't fit in your users personal folder</p> <p>In your <code>~/.bashrc</code>, append the following line: <pre><code>export HF_HOME=/capstor/store/cscs/swissai/a127/homes/$CSCS_USERNAME/hf\n</code></pre></p>"},{"location":"clusters/cscs/#setup-the-environment-on-the-cluster","title":"Setup the environment on the cluster","text":"<p>The terminal will spawn you into the <code>/users/$CSCS_USERNAME</code> directory.</p> <p>When running job, you will need to execute your job inside docker images. This is done by using <code>.toml</code> files that specify which docker image, environment variables are gonne be set when running the job. Create a folder <code>.edf</code> in <code>/users/$CSCS_USERNAME</code>:</p> <pre><code># CSCS login node\n\nmkdir /users/$CSCS_USERNAME/.edf\n</code></pre> <p>Create a <code>/users/$CSCS_USERNAME/.edf/multimodal.toml</code> file:</p> <pre><code>image = \"/capstor/store/cscs/swissai/a127/meditron/docker/multimeditron_latest.sqsh\"\nmounts = [\"/capstor\", \"/iopsstor\", \"/users\"]\n\nwritable = true\n\nworkdir = \"/users/$CSCS_USERNAME/meditron/MultiMeditron\"\n\n[annotations]\ncom.hooks.aws_ofi_nccl.enabled = \"true\"\ncom.hooks.aws_ofi_nccl.variant = \"cuda12\"\n\n[env]\nCUDA_CACHE_DISABLE = \"1\"\nNCCL_NET = \"AWS Libfabric\"\nNCCL_CROSS_NIC = \"1\"\nNCCL_NET_GDR_LEVEL = \"PHB\"\nFI_CXI_DISABLE_HOST_REGISTER = \"1\"\nFI_MR_CACHE_MONITOR = \"userfaultfd\"\nFI_CXI_DEFAULT_CQ_SIZE = \"131072\"\nFI_CXI_DEFAULT_TX_SIZE = \"32768\"\nFI_CXI_RX_MATCH_MODE = \"software\"\nFI_CXI_SAFE_DEVMEM_COPY_THRESHOLD = \"16777216\"\nFI_CXI_COMPAT = \"0\"\n</code></pre> <p>Notice 2 things:</p> <ul> <li>We specify the path to the <code>.sqsh</code> file in the <code>image</code> attribute. This is the image used by the job that stores all of the dependencies.</li> <li>We specify the path to the MultiMeditron repo in the <code>workdir</code> attribute. This is the directory where we spawn when the job is launched.</li> </ul> <p>Note that for other types of job, you will probably require a different image and a different working directory.</p>"},{"location":"clusters/cscs/#launching-job","title":"Launching job","text":"<p>There are 2 types of job that you can launch:</p> <ul> <li>Interactive using <code>srun</code> (which gives you a terminal)</li> <li>Non-interactive using <code>sbatch</code> (which schedule a job)</li> </ul>"},{"location":"clusters/cscs/#interactive-job","title":"Interactive job","text":"<p>On the login node, you can launch an interactive job by executing the following command:</p> <pre><code>srun --time=1:29:59 --partition debug -A a127 --environment=/users/$CSCS_USERNAME/.edf/multimodal.toml --pty bash\n</code></pre> <p>Here is a breakdown of the command:</p> <ul> <li><code>--time</code> is the maximum running time of the job (here, the job runs for 1h30 before it gets killed)</li> <li> <p><code>--partition debug</code> is the node partition in which the job executed. As of 14/08/2025, there are 3 partitions:</p> <ul> <li><code>normal</code>: with a maximum running time of 12 hours and no limit on the number of distributed nodes. This partition is the partition used for non-interactive jobs and long interactive jobs</li> <li><code>debug</code>: with a maximum running time of 1h30 with only one node. This partition is meant for interactive jobs</li> <li><code>xfer</code>: this partition is meant for data transfer and doesn't claim any GPU</li> </ul> </li> </ul> <p>To check if you have been allocated a node, run the following command in another terminal:</p> <pre><code># CSCS login node\n\nsqueue --me --start\n</code></pre> <p>This command will give you a dynamic estimation of the scheduled time (may change as people pass you in the priority queue). Note that this command doesn't output anything if your job has been allocated.</p> <p>Once you have been allocated a job, you will have a terminal inside the allocated node. Make sure that your <code>bash prompt</code> is of the form <code>$CSCS_USERNAME@nidxxxxxx</code> (and not <code>[clariden][$CSCS_USERNAME@clariden-lnxxx]</code>. Run:</p> <pre><code># CSCS job node\n\nnvidia-smi\n</code></pre> <p>to make sure you have 4 GPUs and that you have the driver installed.</p> <p>You can try to launch a training with MultiMeditron by running the following commands:</p> <pre><code>cd MultiMeditron\npip install -e .\ntorchrun --nproc-per-node 4 train.py --config config/config_alignment.yaml\n</code></pre> <p>Once you are done with the job. Type <code>exit</code> to exit the terminal to exit the terminal. This will cancel your job.</p>"},{"location":"clusters/cscs/#non-interactive-job","title":"Non-interactive job","text":"<p>To launch a non-interactive job, you need to create a sbatch script. Create a file called <code>sbatch_train.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name demo-job\n#SBATCH --output /users/$CSCS_USERNAME/meditron/reports/R-%x.%j.out\n#SBATCH --error /users/$CSCS_USERNAME/meditron/reports/R-%x.%j.err\n#SBATCH --nodes 1         # number of Nodes\n#SBATCH --ntasks-per-node 1     # number of MP tasks. IMPORTANT: torchrun represents just 1 Slurm task\n#SBATCH --gres gpu:4        # Number of GPUs\n#SBATCH --cpus-per-task 288     # number of CPUs per task.\n#SBATCH --time 0:59:59       # maximum execution time (DD-HH:MM:SS)\n#SBATCH --environment /users/$CSCS_USERNAME/.edf/multimodal.toml\n#SBATCH -A a127\n\nexport WANDB_DIR=/capstor/store/cscs/swissai/a127/homes/$CSCS_USERNAME/wandb\nexport WANDB_MODE=\"offline\"\nexport HF_TOKEN=$HF_TOKEN\nexport SETUP=\"cd /users/$CSCS_USERNAME/meditron/multimodal/MultiMeditron &amp;&amp; pip install -e .\"\n\nexport CUDA_LAUNCH_BLOCKING=1\necho \"START TIME: $(date)\"\n# auto-fail on any errors in this script\nset -eo pipefail\n# logging script's variables/commands for future debug needs\nset -x\n######################\n### Set enviroment ###\n######################\nGPUS_PER_NODE=4\necho \"NODES: $SLURM_NNODES\"\n######## Args ########\nexport HF_HOME=/capstor/store/cscs/swissai/a127/homes/$CSCS_USERNAME/hf_home\n\n######################\n######################\n#### Set network #####\n######################\nMASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\nMASTER_PORT=6200\n######################\n# note that we don't want to interpolate `\\$SLURM_PROCID` till `srun` since otherwise all nodes will get\n# 0 and the launcher will hang\n#\n# same goes for `\\$(hostname -s|tr -dc '0-9')` - we want it to interpolate at `srun` time\n\nLAUNCHER=\"\n  torchrun \\\n  --nproc_per_node $GPUS_PER_NODE \\\n  --nnodes $SLURM_NNODES \\\n  --node_rank \\$SLURM_PROCID \\\n  --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \\\n  --rdzv_backend c10d \\\n  --max_restarts 0 \\\n  --tee 3 \\\n  \"\n\nexport CMD=\"$LAUNCHER train.py --config config/config_alignment.yaml\"\n\necho $CMD\n\n# srun error handling:\n# --wait=60: wait 60 sec after the first task terminates before terminating all remaining tasks\nSRUN_ARGS=\" \\\n  --cpus-per-task $SLURM_CPUS_PER_TASK \\\n  --jobid $SLURM_JOB_ID \\\n  --wait 60 \\\n  -A a127 \\\n  --reservation=sai-a127\n  \"\n# bash -c is needed for the delayed interpolation of env vars to work\nsrun $SRUN_ARGS bash -c \"$SETUP &amp;&amp; $CMD\"\necho \"END TIME: $(date)\"\n</code></pre> <p>Make sure to replace all the <code>$CSCS_USERNAME</code> by your username and the <code>$HF_TOKEN</code> with your huggingface token. Pay attention to the following parameters:</p> <ul> <li><code>#SBATCH --job-name demo-job</code> sets the job name to <code>demo-job</code></li> <li><code>#SBATCH --nodes 1</code> means that we are claiming one node (of 4 GPUs). You should increase this if you are launching bigger jobs</li> <li><code>#SBATCH --output /users/$CSCS_USERNAME/meditron/reports/R-%x.%j.out</code> and <code>#SBATCH --error /users/$CSCS_USERNAME/meditron/reports/R-%x.%j.err</code> mean that this will create a folder <code>/users/$CSCS_USERNAME/meditron/reports</code> that stores all the job logs</li> <li>Note that here, we execute a training of MultiMeditron with <code>config/config_alignment.yaml</code>, thus you need to make sure that the paths of the dataset are correct</li> <li>Note that the part which follows the <code>#SBATCH</code> commands will be executed on every node</li> </ul> <p>To queue your job, run: bash <pre><code># CSCS login node\n\nsbatch sbatch_train.sh\n</code></pre></p> <p>You can check if your job has been allocated GPUs by running:</p> <p><pre><code># CSCS login node\n\nsqueue --me\n</code></pre> This command gives you the <code>JOBID</code> of the job you have launched</p> <p>Once the job enters the <code>R</code> state (for running), the job is running. You can check the logs of your job by going into the <code>reports</code> directory:</p> <pre><code># Login node\n\ncd /users/$CSCS_USERNAME/meditron/reports/\ntail -f R-%x.%j.err\n</code></pre> <p>where you need to replace <code>R-%x.%j.err</code> by the actual report name.</p> <p>You can either let the job finishes or cancels the job.</p> <pre><code># Login node\n\nscancel $JOBID\n</code></pre> <p>where <code>$JOBID</code>is the <code>JOBID</code> that you get when running <code>squeue --me</code></p>"},{"location":"clusters/cscs/#vscode-connection","title":"VSCode Connection","text":"<p>If you want to join the modern era of computers and have something more involve than a terminal to code (unlike some people), you may want to \"connect\" your visual studio code instance directly to the cluster. This allows to directly modify the code, using the correct environment (so that it doesn't show you half the package as non existent).</p>"},{"location":"clusters/cscs/#procedure","title":"Procedure","text":"<ul> <li>Install the Remote development extension</li> <li>Launch a job on the cluster</li> </ul> <p>You will need the vscode CLI installed on the job you launched.</p> Use prebuilt imageManually install CLI <p>You can use the image that I personally used, you can update your environment file, and use the image at <code>/capstor/store/cscs/swissai/a127/meditron/docker/multimeditron_latest_2.sqsh</code>. With this solution however you'll inherit from all of my python dependencies. If you want to use your own image, you can check the manual installation.</p> <p>If you want to use custom dependency, you'll need to manually install the vscode cli onto you image. To show you an example of it, here's a sample of my <code>Dockerfile</code> responsible for installing the CLI.</p> Sample Dockerfile<pre><code>FROM michelducartier24/multimeditron-apertus\nRUN pip install -U transformers\n\nRUN echo \"\" &gt; /etc/pip/constraint.txt\n\nRUN mkdir -p /workspace/code\nWORKDIR /workspace/code\nRUN curl -Lk 'https://code.visualstudio.com/sha/download?build=stable&amp;os=cli-alpine-arm64' --output vscode_cli.tar.gz\nRUN tar -xf vscode_cli.tar.gz\nRUN mv ./code /usr/bin\nRUN rm -rf /workspace/code\n</code></pre> <ul> <li>Once your job has been launched with vscode CLI installed, it's time to run the code tunnel within the job. Go to the folder of your project and run the following command    <pre><code>cd /path/to/my/awesome/project\ncode tunnel --name=cluster-tunnel\n</code></pre>    This will prompt you to connect to your <code>github</code> account, do so.</li> </ul> <p>Bug in CSCS after update</p> <p>After previous maintainance of CSCS there was a bug where the following code no longer worked. This was due to multiple proxy variable being set. To fix that bug please use the following code:</p> <pre><code>unset {http,https,no}_proxy\nunset {HTTP,HTTPS,NO}_PROXY\n</code></pre> <ul> <li>Finally, open vscode locally on your computer then in the remote extension select the appropriate tunnel and that's it, you are in !</li> </ul>"},{"location":"clusters/cscs_docker/","title":"Building images for the CSCS","text":"<p>This tutorial assumes that you followed the CSCS setup tutorial at cscs.md</p>"},{"location":"clusters/cscs_docker/#claim-a-job-to-build-docker-image","title":"Claim a job to build docker image","text":"<p>Connect to the CSCS login node:</p> <pre><code>ssh clariden\n</code></pre> <p>Claim a job:</p> <pre><code># On the login node\n\nsrun --time 11:59:59 -p normal -A a127 --pty bash\n</code></pre> <p>Make sure to put a big timeout as building docker images can take a lot of time</p>"},{"location":"clusters/cscs_docker/#configure-podman-storage","title":"Configure podman storage","text":"<p>In order to use podman on alps, you need to create a valid container storage configuration file at <code>$HOME/.config/containers/storage.conf</code>. In pratice you need to crete the following file at <code>/users/&lt;USERNAME&gt;/.config/containers/storage.conf</code>. </p> <pre><code>[storage]\ndriver = \"overlay\"\nrunroot = \"/dev/shm/$USER/runroot\"\ngraphroot = \"/dev/shm/$USER/root\"\n\n[storage.options.overlay]\nmount_program = \"/usr/bin/fuse-overlayfs-1.13\"\n</code></pre>"},{"location":"clusters/cscs_docker/#building-the-image","title":"Building the image","text":"<p>Create your Dockerfile and name it <code>Dockerfile</code>. For example, this is the Dockerfile used to run axolotl:</p> <pre><code>FROM nvcr.io/nvidia/pytorch:24.07-py3\n\n# setup\nRUN apt-get update &amp;&amp; apt-get install python3-pip python3-venv -y\nRUN pip install --upgrade pip setuptools\n\n# Axolotl installs\nRUN MAX_JOBS=24 pip install flash-attn==2.6.2 --no-build-isolation\nRUN pip install trl==0.9.6\nRUN pip install fschat@git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe\nRUN pip install deepspeed==0.14.4\n\nRUN MAX_JOBS=8 TORCH_CUDA_ARCH_LIST=\"9.0\" pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers\nRUN pip install transformers@git+https://github.com/huggingface/transformers.git@026a173a64372e9602a16523b8fae9de4b0ff428\n\nCOPY axolotl/ /workspace/axolotl\nWORKDIR /workspace/axolotl\nRUN pip install -e .\n</code></pre> <p>Note that this Dockerfile assumes that you have the axolotl repository cloned in <code>path/to/root_directory</code></p> <p>Run:</p> <pre><code># Recommended: change the tag from my_awesome_image to a more meaningful one\n\npodman build -f /path/to/root_directory/Dockerfile -t my_awesome_image path/to/root_directory\n</code></pre> <p>You can change the base docker image (the <code>FROM</code> value) to have a more recent version of the CUDA driver. You can check the available tags here.</p> <p>IMPORTANT NOTE: Note that the CSCS cluster uses an ARM64 architecture. Make sure that you are building your packages for this architecture. Some libraries like VLLM may be trickier to install as they rely on low-level optimization.</p>"},{"location":"clusters/cscs_docker/#export-the-docker-image-to-a-sqsh-file","title":"Export the docker image to a .sqsh file","text":"<p>Run the following command, you can change the <code>my_custom_image.sqsh</code> file name to a more meaningful one.</p> <pre><code>enroot import -o /capstor/store/cscs/swissai/a127/meditron/docker/my_custom_image.sqsh podman://localhost/my_awesome_image:latest\n</code></pre> <p>Set the correct permissions:</p> <pre><code>setfacl -b /capstor/store/cscs/swissai/a127/meditron/docker/my_custom_image.sqsh\nchmod +r /capstor/store/cscs/swissai/a127/meditron/docker/my_custom_image.sqsh\n</code></pre>"},{"location":"clusters/cscs_docker/#use-your-new-docker-image","title":"Use your new Docker image","text":"<p>To use your new Docker image, create a new toml file in <code>$HOME/.edf</code> (in this example we name it <code>example.toml</code>):</p> <pre><code>image = \"/capstor/store/cscs/swissai/a127/meditron/docker/my_custom_image.sqsh\"\nmounts = [\"/capstor\", \"/iopsstor\", \"/users\"]\n\nwritable = true\n\n# Uncomment this to set a particular working directory\n# workdir = \"/path/to/workdir\"\n\n[annotations]\ncom.hooks.aws_ofi_nccl.enabled = \"true\"\ncom.hooks.aws_ofi_nccl.variant = \"cuda12\"\n\n[env]\nCUDA_CACHE_DISABLE = \"1\"\nNCCL_NET = \"AWS Libfabric\"\nNCCL_CROSS_NIC = \"1\"\nNCCL_NET_GDR_LEVEL = \"PHB\"\nFI_CXI_DISABLE_HOST_REGISTER = \"1\"\nFI_MR_CACHE_MONITOR = \"userfaultfd\"\nFI_CXI_DEFAULT_CQ_SIZE = \"131072\"\nFI_CXI_DEFAULT_TX_SIZE = \"32768\"\nFI_CXI_RX_MATCH_MODE = \"software\"\nFI_CXI_SAFE_DEVMEM_COPY_THRESHOLD = \"16777216\"\nFI_CXI_COMPAT = \"0\"\n</code></pre> <p>Note the line starting with <code>image =</code></p> <p>Then, to claim an interactive job with this image:</p> <pre><code>srun --time=1:29:59 --partition debug -A a127 --environment=$HOME/.edf/example.toml --pty bash\n</code></pre> <p>For non interactive job. Look for the <code>--environment</code> argument in the srun arguments and change it to your desired <code>.toml</code> file (here <code>example.toml</code></p>"},{"location":"clusters/intro_cscs/","title":"Introduction to the CSCS cluster","text":"<p>The CSCS cluster is a cluster built for Swiss researchers that provides national high-performance computing. This cluster should be used for model training and inference:</p> <ul> <li>To connect to the cluster: see this</li> <li>To build images for the cluster: see this</li> </ul> <p>Here is the overview of the architecture:</p> <pre><code>architecture-beta\n    group api[CSCS cluster]\n    group nodes[GPU nodes] in api\n\n    service laptop(mids:laptop)[Your Laptop]\n    service storage(database)[Storage] in api\n    service login(server)[Login node] in api\n\n    service node1(server)[nid0000] in nodes\n    service node2(server)[nid0001] in nodes\n    service node3(server)[nid0002]in nodes\n    service node4(server)[nid0003]in nodes\n\n    junction junctionStorage in api\n\n    laptop:R --&gt; L:login\n\n    login:B -- T:storage\n    login:R --&gt; L:node1{group}\n\n    junctionStorage:T -- B:node1{group}\n    storage:R -- L:junctionStorage\n\n    node1:T -- B:node2\n    node1:R -- L:node3\n    node2:R -- L:node4\n    node3:T -- B:node4</code></pre>"},{"location":"clusters/rcp/","title":"Connecting to RCP","text":""},{"location":"clusters/rcp/#1-pre-setup-access-to-scratch-and-cluster","title":"1. Pre-setup (access to scratch and cluster)","text":"<p>Please ask Mark or Peter to add you to the corresponding groups (send a message on Slack to get a faster answer). You can check your groups at https://groups.epfl.ch/</p>"},{"location":"clusters/rcp/#2-setting-up-credentials","title":"2. Setting-up credentials","text":"<p>This part makes sure that you have access to GitHub, wandb and huggingface from the cluster. If it's not already done, create an account on those platforms! To setup the credentials, we must access the scratch in <code>haas001.rcp.epfl.ch</code> using ssh. The password is your GASPAR credentials: <pre><code>ssh $GASPAR@haas001.rcp.epfl.ch\n</code></pre> For this part, every command will be done from the ssh terminal</p> <p>Go in the scratch directory (<code>/mnt/mlo/scratch</code>): <pre><code># SSH terminal\n\ncd /mnt/light/scratch/users\nmkdir $GASPAR\n</code></pre></p>"},{"location":"clusters/rcp/#wandb-and-huggingface-credentials","title":"WANDB and HuggingFace credentials","text":"<p>We will store the API keys within our directory in a folder that only our user will have access to. Both the wandb and Hugging Face API keys will be stored in a .txt file within this protected folder.</p> <p><pre><code># SSH terminal\n\ncd /mnt/light/scratch/users/$GASPAR_USER\nmkdir keys\ncd keys\ntouch hf_key.txt\ntouch wandb_key.txt\nchmod 700 -R ../keys/\n</code></pre> * For Huggingface: you can create a new access token at https://huggingface.co/settings/tokens. Put this token in the file <code>hf_key.txt</code> * For WANDB: you can access your tokens at https://wandb.ai/settings. Scroll down to \"API keys\". Put this token in the file <code>wandb_key.txt</code></p>"},{"location":"clusters/rcp/#github-credentials","title":"Github credentials","text":"<p>To carry out the automatic login to GitHub, we will need to store our git identification (.gitconfig) and our access credentials (.git-credentials), which in this case we will do using a Personal Access Token.</p> <p>To do this, we will need to set the environment variable <code>$HOME</code> to the personal folder we have created and activate the credential helper that will store our access credentials. <pre><code># SSH terminal\n\nexport HOME=/mnt/light/scratch/users/$GASPAR_USER\ngit config --global credential.helper store\n</code></pre></p> <p>Then we will configure our git identification, specifying a username and email address. <pre><code># SSH terminal\n\ngit config --global user.name \"GITHUB_USERNAME\"\ngit config --global user.email \"MY_NAME@example.com\"\n</code></pre></p> <p>Create a Personal access token. Select <code>Generate new token</code> and the <code>classic</code> option. Give every permissions to this token.</p> <p>Finally, we will execute an action that requires our identification on GitHub to enter our access credentials and store them (e.g. Clone a private repository). When prompted for the password, we will enter the Personal Access Token that we created: <pre><code># SSH terminal\n\ngit clone https://github.com/EPFLiGHT/MultiMeditron.git\n</code></pre> If you were able to clone the repo, then your setup is correct.</p>"},{"location":"clusters/rcp/#remote-vscode-configuration","title":"Remote VSCode configuration","text":"<p>We will store the configurations related to VSCode in a folder in the scratch volume so that we don't have to download them every time we start a new container. <pre><code># SSH terminal\n\nmkdir /mnt/light/scratch/users/$GASPAR_USER/.vscode-server\n</code></pre></p>"},{"location":"clusters/rcp/#3-setup-runai-and-kubectl-on-your-machine","title":"3. Setup runai and kubectl on your machine","text":"<p>IMPORTANT: The setup below was tested on macOS with Apple Silicon. If you are using a different system, you may need to adapt the commands. For Windows, we have no experience with the setup and thereby recommend WSL (Windows Subsystem for Linux) to run the commands. If you choose WSL, you should choose the commands as if you were running Linux.</p> <p>Install kubectl</p> <pre><code># Your terminal (either WSL, Linux or Mac)\n\ncurl -LO \"https://dl.k8s.io/release/v1.29.6/bin/darwin/arm64/kubectl\"\n# Linux: curl -LO \"https://dl.k8s.io/release/v1.29.6/bin/linux/amd64/kubectl\"\n\n# Give it the right permissions and move it.\nchmod +x ./kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl\nsudo chown root: /usr/local/bin/kubectl\n</code></pre> <p>Setup the kube config file: Take our template file kubeconfig.yaml as your config in the home folder ~/.kube/config. Note that the file on your machine has no suffix.</p> <pre><code># Your terminal\n\nmkdir ~/.kube/\ncurl -o  ~/.kube/config https://raw.githubusercontent.com/epfml/getting-started/main/kubeconfig.yaml\n</code></pre> <p>Install the run:ai CLI for RCP (two RCP clusters):</p> <pre><code># Your terminal\n\n# Download the CLI from the link shown in the help section.\n# for Linux: replace `darwin` with `linux`\nwget --content-disposition https://rcp-caas-prod.rcp.epfl.ch/cli/darwin\n# Give it the right permissions and move it.\nchmod +x ./runai\nsudo mv ./runai /usr/local/bin/runai\nsudo chown root: /usr/local/bin/runai\n</code></pre>"},{"location":"clusters/rcp/#4-login","title":"4. Login","text":"<p>The RCP is organized into a 3 level hierarchy. The department is the laboratory (e.g. LIGHT or MLO). The projects determine which scratch (aka persistent storage) we have access to. Note that you should choose the SSO option when executing <code>runai login</code>.</p> <pre><code># Your terminal\n\nrunai config cluster rcp-caas-prod\nrunai login\nrunai list project\nrunai config project light-$GASPAR\n</code></pre>"},{"location":"clusters/rcp/#5-submit-a-job","title":"5. Submit a job","text":"<p>Time to test if we can submit a job! This command will allocate 1 GPU from the cluster and \"sleep\" to infinity (meaning that it will do essentially nothing) </p> <pre><code># Your terminal\n\nrunai submit \\\n  --name meditron-basic \\\n  --image registry.rcp.epfl.ch/multimeditron/basic:latest-$GASPAR\\\n  --pvc light-scratch:/mloscratch \\\n  --large-shm \\\n  -e NAS_HOME=/mloscratch/users/$GASPAR \\\n  -e HF_API_KEY_FILE_AT=/mloscratch/users/$GASPAR/keys/hf_key.txt \\\n  -e WANDB_API_KEY_FILE_AT=/mloscratch/users/$GASPAR/keys/wandb_key.txt \\\n  -e GITCONFIG_AT=/mloscratch/users/$GASPAR/.gitconfig \\\n  -e GIT_CREDENTIALS_AT=/mloscratch/users/$GASPAR/.git-credentials \\\n  -e VSCODE_CONFIG_AT=/mloscratch/users/$GASPAR/.vscode-server \\\n  --backoff-limit 0 \\\n  --run-as-gid 84257 \\\n  --node-pool h100 \\\n  --gpu 1 \\\n  -- sleep infinity\n</code></pre> <p>Note: If you have issue with the job not being launched (after doing a <code>describe</code>), ensure that there is such an image in the registry. You can build your image following the docker tutorial.</p> <p>Explanation:</p> <ul> <li><code>name</code> is the name of the job</li> <li><code>image</code> is the link to the docker image that will be attached to the cluster. Please note that you may need to change the image path if you pushed your image on another link. See Building Docker image for the RCP</li> <li><code>pvc</code> determines which scratch will be mounted to the job. The argument is of the form: <code>name_of_the_scratch:/mount/path/to/scratch</code>. Here the we are mounting the scratch named <code>light-scratch</code> to the local path <code>/mloscratch</code> This is part may cause an error because of the LIGHT migration </li> <li><code>gpu</code> is the number of GPU that you want to claim for this job (larger amount of GPU will be harder to get as ressources are limited)</li> </ul> <p>We can check the outputs of our container and the status of the job using the following commands respectively. <pre><code># Your terminal\n\nrunai logs meditron-basic\nrunai describe job meditron-basic\n</code></pre></p> <p>To end a job, run the command: <pre><code># Your terminal\n\nrunai delete job meditron-basic\n</code></pre></p> <p>You can access your job by doing <pre><code># Your terminal\n\nrunai bash meditron-basic\n</code></pre> You should see a terminal opening. Enter the following command in your new terminal to ensure that you have indeed a GPU:  <pre><code># Job terminal\n\nnvidia-smi\n</code></pre></p> <p>Once you are done, run the following command to delete the job: <pre><code># Your terminal\n\nrunai delete job meditron-basic\n</code></pre></p>"},{"location":"clusters/rcp/#6-vscode-connection","title":"6. VSCode connection","text":""},{"location":"clusters/rcp/#mac-and-linux","title":"Mac and Linux","text":"<p>Once we have the container running on a node of the RCP cluster, we can attach to it in VSCode. To do this, we need to have the following extensions installed:</p> <ul> <li>Kubernetes</li> <li>Dev containers</li> </ul> <p>From the Kubernetes menu, we can see the IC and the RCP Cluster. We will enter the menu of the RCP Cluster -&gt; Workloads -&gt; Pods and we will see our container with a green indicator showing that it is running. Right-clicking on it will give us the option to \"Attach to Visual Studio\". Upon clicking, the editor will open in a new window within the container. We are then invited to open a folder, it should be our personal folder (<code>/mloscratch/users/$GASPAR</code>) by default, select it. When opening a new terminal, we should find ourselves directly in our personal folder, if needed we can move there with <code>cd</code> in the terminal. We can install new extensions on VS code, and they will be saved for future sessions.</p>"},{"location":"clusters/rcp/#windows-wsl-connection","title":"Windows (WSL connection)","text":"<p>For WSL setup, you will need kubernetes on your Windows host because VSCode is going to look for it on the host (and not in WSL).</p> <p>In Windows terminal (not WSL), run:</p> <pre><code># Windows terminal\n\ncurl.exe -LO \"https://dl.k8s.io/release/v1.32.0/bin/windows/amd64/kubectl.exe\"\n</code></pre> <p>Create a folder <code>~/.kube</code> in Windows: <pre><code># Windows terminal\n\nmkdir ~/.kube/\n</code></pre></p> <p>In WSL, claim a job and copy the kube configuration file from WSL to Windows <pre><code># WSL terminal\n\nrunai submit \\\n  --name meditron-basic \\\n  --image registry.rcp.epfl.ch/multimeditron/basic:latest-$GASPAR\\\n  --pvc light-scratch:/mloscratch \\\n  --large-shm \\\n  -e NAS_HOME=/mloscratch/users/$GASPAR \\\n  -e HF_API_KEY_FILE_AT=/mloscratch/users/$GASPAR/keys/hf_key.txt \\\n  -e WANDB_API_KEY_FILE_AT=/mloscratch/users/$GASPAR/keys/wandb_key.txt \\\n  -e GITCONFIG_AT=/mloscratch/users/$GASPAR/.gitconfig \\\n  -e GIT_CREDENTIALS_AT=/mloscratch/users/$GASPAR/.git-credentials \\\n  -e VSCODE_CONFIG_AT=/mloscratch/users/$GASPAR/.vscode-server \\\n  --backoff-limit 0 \\\n  --run-as-gid 84257 \\\n  --node-pool h100 \\\n  --gpu 1 \\\n  -- sleep infinity\n\ncp ~/.kube/config /mnt/c/Users/$WINDOWS_USERNAME/.kube/config\n</code></pre></p> <p>Open VSCode. Install this extension: https://marketplace.visualstudio.com/items?itemName=mtsmfm.vscode-k8s-quick-attach. </p> <p>To attach VSCode to your job: Go to View -&gt; Command Palette (or Ctrl+Shift+P), search for \"k8s quick attach: Quick attach k8s Pod\" -&gt; rcp-caas -&gt; runai-mlo-GASPAR -&gt; meditron-basic-0-0 -&gt; /mloscratch/users/$GASPAR_USER.</p>"},{"location":"clusters/rcp/#vscode-troubleshooting","title":"VSCode Troubleshooting","text":"<p>If you encounter the following error: </p> <p>Run: <pre><code># WSL terminal\n\nrunai login\ncp ~/.kube/config /mnt/c/Users/$WINDOWS_USERNAME/.kube/config\n</code></pre> And try to attach VSCode again</p>"},{"location":"clusters/rcp/#more-ressources","title":"More ressources","text":"<ul> <li>EPFL RCP Wiki</li> <li>runai submit Documentation</li> </ul>"},{"location":"clusters/rcp_docker/","title":"Building Docker image for the RCP","text":"<p>To build a Docker image for the RCP, we will use the LiGHT cluster template.</p> <p>Prerequisites: - You need docker on your computer - A Linux environment (as we are building docker images that target Linux) - Good specs, building docker images require lots of resources in RAM and can take lots of space in your disk. Make sure that your computer is \"good enough\"</p>"},{"location":"clusters/rcp_docker/#setup","title":"Setup","text":"<p>On your machine:</p> <pre><code>git clone https://github.com/EPFLiGHT/LiGHT-cluster-template.git\n</code></pre> <p>The repository contains tutorials and reproducibility scripts for many clusters, but we will only focus on the RCP in thus tutorial.</p> <pre><code>cd LiGHT-cluster-template/installation/docker-amd64-cuda\n</code></pre> <p>We will build the docker images in the following way:</p> <ol> <li>Build a generic docker image that contains all the dependencies (NVidia drivers, pip, uv, git, etc.)</li> <li>Build many user docker images extending this generic image with the right permisions</li> </ol> <p>This folder contains a <code>template.sh</code> file which contains all the available functions</p>"},{"location":"clusters/rcp_docker/#creating-a-project-on-the-epfl-registry-optional","title":"Creating a project on the EPFL registry (Optional)","text":"<p>EPFL has a registry of docker images which works in a similar way to the Docker hub. This registry is used to store docker images and is only accessible through the EPFL VPN.</p> <p>Connect to registry.rcp.epfl.ch with the VPN. To push new docker images, you need to create a Project. At the time of this tutorial, some projects have already been created like multimeditron and you can ask Michael to add you to the project so that you can also push Docker images to this project.</p> <p>You also have the possibility of creating your own project by clicking on \"New project\". </p> <p>IMPORTANT: Beware that if you choose to create your own project, you will have to change the link of the docker images in the scripts accordingly (mainly replacing multimeditron/basic by the right path)</p>"},{"location":"clusters/rcp_docker/#generating-the-env-file","title":"Generating the .env file","text":"<p>Run the following command:</p> <pre><code>./template.sh env\n</code></pre> <p>This command create a (hidden) <code>.env</code> file which looks like this:</p> <pre><code># All user-specific configurations are here.\n\n## For building:\n# Which docker and compose binary to use\n# docker and docker compose in general or podman and podman-compose for CSCS Clariden\nDOCKER=docker\nCOMPOSE=\"docker compose\"\n# Use the same USRID and GRPID as on the storage you will be mounting.\n# USR is used in the image name and must be lowercase.\n# It's fine if your username is not lowercase, jut make it lowercase.\nUSR=john\nUSRID=1000\nGRPID=984\nGRP=users\n# PASSWD is not secret,\n# it is only there to avoid running password-less sudo commands accidentally.\nPASSWD=john\n# LAB_NAME will be the first component in the image path.\n# It must be lowercase.\nLAB_NAME=users\n\n#### For running locally\n# You can find the acceleration options in the compose.yaml file\n# by looking at the services with names dev-local-ACCELERATION.\nPROJECT_ROOT_AT=/path/to/LiGHT-cluster-template\nACCELERATION=cuda\nWANDB_API_KEY=\n# PyCharm-related. Fill after installing the IDE manually the first time.\nPYCHARM_IDE_AT=\n\n\n####################\n# Project-specific environment variables.\n## Used to avoid writing paths multiple times and creating inconsistencies.\n## You should not need to change anything below this line.\nPROJECT_NAME=template-project-name\nPACKAGE_NAME=template_package_name\nIMAGE_NAME=${LAB_NAME}/${USR}/${PROJECT_NAME}\nIMAGE_PLATFORM=amd64-cuda\n# The image name includes the USR to separate the images in an image registry.\n# Its tag includes the platform for registries that don't hand multi-platform images for the same tag.\n# You can also add a suffix to the platform e.g. -jax or -pytorch if you use different images for different environments/models etc.\n</code></pre> <p>Edit this file by setting <code>LAB_NAME</code> to the name of the project you have given in the previous step (multimeditron if you choose to push your docker images there). Set the <code>PROJECT_NAME</code> to a meaningful name to group the docker images that belong to the same project. Here we will choose to set it up to <code>basic</code>. Docker images with the same <code>PROJECT_NAME</code> will appear together in the <code>registry.rcp.epfl.ch</code> interface.</p> <p>We also change the <code>IMAGE_NAME</code> to another template for organization purpose.</p> <p>Edit the corresponding lines to the following lines:</p> <pre><code>LAB_NAME=multimeditron # Or the name you have chosen\nPROJECT_NAME=basic # Optional: Change this to a more meaningful name (e.g. vllm or axolotl for instance)\nIMAGE_NAME=${LAB_NAME}/${PROJECT_NAME}\n</code></pre>"},{"location":"clusters/rcp_docker/#building-the-generic-docker-image","title":"Building the generic docker image","text":"<p>Beware that this docker image uses the <code>Dockerfile</code> to build. You may need to change the base Docker image in <code>compose-base.yaml</code>:</p> <pre><code>BASE_IMAGE: nvcr.io/nvidia/pytorch:pytorch:24-07-py3 # (Optional: Change this tag to a newer version)\n</code></pre> <p>You can change this line to another newer version that you can find here. This will change the version of the NVidia driver that many libraries rely on. Some of the latest features may only be compatible with the newer version of the drivers (for instance the flash-attn library).</p> <p>To build the generic docker image run the following command:</p> <pre><code>./template.sh build_generic\n</code></pre> <p>Useful commands:</p> <p><code>docker ps</code>: To list all the docker images on your machine. Search for the one that you just built, it should have the name that you set up in <code>IMAGE_NAME</code>. So if we are using multimeditron, it will be tagged as <code>multimeditron/basic:amd64-cuda-root-latest</code> and <code>multimeditron/basic:amd64-cuda-root-&lt;some git commit hash&gt;</code></p> <p><code>docker run --rm -it --entrypoint bash multimeditron/basic:amd64-cuda-root-latest</code>: To bash into your new docker image and test if you have everything installed correctly.</p>"},{"location":"clusters/rcp_docker/#building-the-user-docker-images","title":"Building the user docker images","text":"<p>Connect to people.epfl.ch and search for the user you want to build an image. You need the <code>Username</code> and the <code>UID</code> fields to build the user docker image</p> <p>Connect to groups.epfl.ch and search for light-scratch in \"All groups\". You need the <code>GID</code> field.</p> <p>To build user docker images, you need to edit the <code>.env</code> file. You need to change the following attributes:</p> <pre><code>GRPID=&lt;GID of the light-scratch&gt;\n\nUSRID=&lt;UID of the user&gt;\nUSR=&lt;Username of the user&gt;\nPASSWD=&lt;Username of the user&gt;\n</code></pre> <p>To build the user docker image, run:</p> <pre><code>./template.sh build_user\n</code></pre>"},{"location":"clusters/rcp_docker/#push-the-docker-images","title":"Push the docker images","text":"<p>Login to the registry:</p> <pre><code>docker login\n</code></pre> <p>The login informations are your EPFL login</p> <pre><code>./template.sh push_generic RCP\n./template.sh push_user RCP\n</code></pre>"}]}