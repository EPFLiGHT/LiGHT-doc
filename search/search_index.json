{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to LiGHT","text":"<p>LiGHT stands for Laboratory for Intelligent Global Health and Humanitarian Response Technologies</p> <p>If you are new to LiGHT, start by reading the Getting Started tutorial</p>"},{"location":"#projects","title":"Projects","text":"<p>If you are a student interested in joining LiGHT (or just curious about what we do), you can explore our ongoing projects in the Projects section</p>"},{"location":"#clusters","title":"Clusters","text":"<p>Modern LLMs requires an unresonable amount of computational resources. As such, to develop and test our models, we have access to multiple clusters. If you are part of the laboratory, and want to learn more about how to use them, check the Clusters section</p> <p>Please refer to your supervisor or the lab admin for any questions regarding access to the clusters.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>This documentation is a work in progress. If you have any suggestions or want to contribute, please reach out to the lab admin or open an issue on our GitHub repository</p>"},{"location":"about/","title":"Who We Are","text":"<p>We, at LiGHT, are a team of passionate individuals committed to building and developing AI systems that improve healthcare innovatively and ethically. We are a students led organization based at the Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL) in Switzerland.</p>"},{"location":"clusters/","title":"Connecting to the clusters","text":"<p>Throughout your work at LiGHT, you will need to run some heavy computations (whether for inference or training). LiGHT has access to many different services to run those computations</p> <ul> <li>EPFL RCP: a cluster hosted at EPFL. This cluster should be used for inference, small training and experiments on small models</li> <li>SwissAI CSCS: a cluster maded by SwissAI. This cluster should be used for larger training on bigger models.</li> </ul>"},{"location":"contributing/","title":"How to contribute to LiGHT docs","text":""},{"location":"contributing/#guidelines","title":"Guidelines","text":"<p>To contribute, you can suggest changes and open pull requests at our GitHub repo</p> <p><code>LiGHT-doc</code> uses <code>mkdocs</code>. <code>mkdocs</code> is a tool that automatically build documentation from markdown files.</p> <p>For full documentation visit mkdocs.org.</p>"},{"location":"contributing/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> <p>Every markdown files are written in the <code>docs</code> directory. </p>"},{"location":"contributing/#workflow","title":"Workflow","text":"<ul> <li> <p>Update your relevant files in the <code>docs</code> folder</p> </li> <li> <p>Test your local changes by running <pre><code>mkdocs serve\n</code></pre> and accessing <code>localhost:8000</code> on your browser</p> </li> <li> <p>Push your changes to some branch <pre><code>git checkout -b [branch name]\ngit add [files]\ngit commit -m [Commit message]\ngit push\n</code></pre></p> </li> <li> <p>Open a pull request to the <code>main</code> branch on GitHub</p> </li> <li> <p>Once the pull request is merged on <code>main</code>, changes should be visible at https://light-yalepfl.github.io/LiGHT-doc/</p> </li> </ul>"},{"location":"gettingstarted/","title":"Getting started","text":"<p>Welcome to LiGHT! </p> <p>For your projects at LiGHT, you will probably need GPUs (for model inference or training). To get access to those resources, check the dedicated section</p>"},{"location":"projects/","title":"Projects for Fall 2025","text":"<ul> <li> <p> MMORE</p> <p>MMORE is our Python library for a scalable multimodal pipeline for processing, indexing, and querying multimodal documents. It is used for retrieval augmented generation (RAG) applications.</p> <p> See MMORE information</p> </li> <li> <p> MIRAGE</p> <p>MIRAGE is a platform designed to streamline the processing of datasets using generative models.</p> <p> See MIRAGE information</p> </li> <li> <p> Polyglot</p> <p>Polyglot Meditron is a project aimed at evaluating and enhancing the multilingual capabilities of our Meditron model.</p> <p> See Multilingual Meditron information</p> </li> <li> <p> LiGHT Bootcamp</p> <p>Improve the content of the MOOC on AI applied to healthcare</p> <p> See LiGHT Bootcamp information</p> </li> <li> <p> MultiMeditron</p> <p>Improve Meditron's multimodal capabilities by enabling it to process and understand multiple modalities.</p> <p> See MultiMeditron information</p> </li> <li> <p> Quantisation of Medical LLMs</p> <p>Explore model quantisation in practice and document the results in a reproducible way.</p> <p> See Quantisation of Medical LLMs information</p> </li> <li> <p> Distillation of Medical LLMs</p> <p>Explore knowledge distillation for language models, with an emphasis on comparing different distillation strategies, data choices, and model architectures.</p> <p> See Distillation of Medical LLMs information</p> </li> <li> <p> Meditron reasoning</p> <p>Integrate reasoning through unsupervised reinforcement learning into Meditron aiming to further elevate its performance and decision-making abilities.</p> <p> See Meditron reasoning information</p> </li> <li> <p> MOOVE</p> <p>MOOVE (Massive Open Online Validation and Evaluation) is a large-scale, participatory evaluation platform designed to collect, structure, and analyze expert feedback on the outputs of clinical large language models (LLMs).</p> <p> See MOOVE information</p> </li> </ul>"},{"location":"projects/#mmore-mirage","title":"MMORE &amp; Mirage","text":"<p>This project is supervised by Fabrice Nemo</p> <p>MMORE stands for Massive Multimodal Open RAG &amp; Extraction, it is our Python library for a scalable multimodal pipeline for processing, indexing, and querying multimodal documents. MIRAGE stands for Multimodal Intelligent Reformatting and Augmentation Generation Engine, it is our advanced platform designed to streamline the processing of datasets using generative models.</p> <p>The aim of these two projects is to work on maintaining the library: solve the issues raised by the community, fix bugs, make new features that could be useful and challenging for students (would be suggested by students or by Fabrice if the idea comes up as important enough).</p>"},{"location":"projects/#polyglot-meditron","title":"Polyglot Meditron","text":"<p>This project is supervised by Fabrice Nemo</p> <p>Speaking English is nice, most content online is in English. Having a performant LLM for medical tasks formulated in English is useful. But not enough! In low-resource settings and even in most places of the globe, people usually prefer using their first language rather than English.</p> <p>This project aims at making Meditron models more proficient in other languages, with a focus on low-resource languages (current focus on Amharic, Hindi, Swahili, Tamil, eventually also Arabic, Bembe, French, Kinyarwanda, Luo, Nyanja, Twi, Urdu). In written and spoken speech. Work is needed, since having a polyglot base model is generally not enough: popular models do not have a focus on low-resource languages, and there is also a need to make sure to teach the model non-English medical terminology.</p>"},{"location":"projects/#light-ai-bootcamp","title":"LiGHT AI Bootcamp","text":"<p>This project is supervised by Fabrice Nemo</p> <p>Teaching the basics of AI applied to healthcare. We already have a MOOC (almost) ready. The target audience is healthcare workers and computer scientists in Africa, who would be following the MOOC with human mentoring provided by LiGHT. Our work in LiGHT is to improve the content of the MOOC so that students learn better, and mentor students in Africa, guide them throughout their completion of the bootcamp. Students may work on this project either as a side project (1 or 2 hours per week of mentoring) or as a full time semester/optional project (for instance for making deeper research on how to improve the MOOC with evidence from educational science, for developing more evaluation content\u2026 To be discussed with Fabrice).</p>"},{"location":"projects/#multimeditron","title":"MultiMeditron","text":"<p>This project is co-supervised by David Sasu, Lars Klein, Frabrice Nemo and Arianna Francesconi</p> <p>This project aimed at improving Meditron multimodal capabilities. Healthcare data is often multimodal, combining text, images, signals, and other data types. Enabling Meditron to process and understand multiple modalities can significantly enhance its performance in medical applications.</p> <p>The goal of this project is:</p> <ul> <li>Adapting the codebase of Meditron to make it have a multimodal architecture, adapted to new modalities (for now the codebase only supports images)</li> <li>Making and improving the \"expert\" models that process the modalities and make embeddings fed to Meditron.</li> </ul>"},{"location":"projects/#quantisation-of-medical-llms","title":"Quantisation of Medical LLMs","text":"<p>This project is supervised by Lars Klein</p> <p>This project focuses on exploring model quantisation in practice and documenting the results in a reproducible way. The goal is to gain hands-on experience with commonly used quantisation tools and to produce clear notes and artifacts that capture their behavior, trade-offs, and performance characteristics.</p> <p>Tasks:</p> <ul> <li>Select and evaluate 1-3 quantisation tools (e.g. bitsandbytes, llama.cpp)</li> <li>Apply quantisation to one or more models and document the process in detail, including:</li> <li>exact steps taken,</li> <li>runtime of the quantisation process,</li> <li>resulting model size and size reduction.</li> <li>Run the quantised models through benchmarks and record:</li> <li>inference speed,</li> <li>resource usage,</li> <li>any observable changes in output quality or behavior.</li> </ul> <p>Required Experience:</p> <ul> <li>Basic Python programming</li> <li>Familiarity with running machine learning models from the command line or in scripts</li> </ul>"},{"location":"projects/#distillation-of-medical-llms","title":"Distillation of Medical LLMs","text":"<p>This project is supervised by Lars Klein and Arianna Francesconi</p> <p>This project explores knowledge distillation for language models, with an emphasis on comparing different distillation strategies, data choices, and model architectures. The aim is to better understand how teacher selection, loss functions, and training data affect the performance and efficiency of distilled student models.</p> <p>Tasks:</p> <ul> <li>Identify suitable teacher models and, if needed, construct datasets for intermediate representations (e.g. activations).</li> <li>Implement and compare different distillation losses, including:</li> <li>logit-based distillation,</li> <li>MiniLM-style objectives</li> <li>Experiment with different training datasets, such as:</li> <li>general-purpose corpora,</li> <li>task-specific datasets.</li> <li>Benchmark distilled models on relevant evaluation tasks and performance metrics.</li> <li>Explore distillation across architectures, including heterogeneous setups (e.g. LFM-style distillation between models such as Apertus and Meditron).</li> </ul> <p>Required Experience:</p> <ul> <li>Solid Python programming</li> <li>Familiarity with training and evaluating neural networks</li> <li>Basic understanding of language models and knowledge distillation techniques</li> </ul>"},{"location":"projects/#meditron-reasoning","title":"Meditron reasoning","text":"<p>This project is supervised by Guillaume Boy\u00e9 and Lars Klein</p> <p>Reasoning has been a significant breakthrough in advancing the capabilities of large language models in recent years. It has consistently demonstrated its ability to enhance decision-making processes within these systems. The objective of this project is to integrate reasoning through unsupervised reinforcement learning into Meditron aiming to further elevate its performance and decision-making abilities.</p> <p>Completed:</p> <ul> <li>Integrated VERL on the cluster with distributed training on multi-node with appropriate docker image</li> <li>Docker image for SGLang inference</li> <li>LLM-as-a-judge based reward</li> <li>Distributed setup</li> <li>Prototype dataset and prototype reward function</li> <li>Prototype support for multiturn and tooling for python execution</li> </ul> <p>Possible Tasks:</p> <ul> <li>Experiment with new datasets and reward modeling for reasoning tasks to enhance model generation</li> <li>Explore additional RL algorithms and architecture for improving capabilities (multi-agent setup)</li> <li>Expand the tool based and introduce RAG system to improve the observability of the reasoning</li> <li>Benchmark model performance on complex tasks</li> </ul> <p>(Required) Experience:</p> <ul> <li>Strong knowledge of Python, PyTorch experience is a plus</li> <li>Experience on distributed infrastructure using SLURM, working with server is a plus</li> <li>Linux knowledge (for building Docker image, GLHF)</li> <li>Knowledge of reward modeling is a plus</li> </ul>"},{"location":"projects/#moove-massive-open-online-validation-and-evaluation","title":"MOOVE: Massive Open Online Validation and Evaluation","text":"<p>This project is supervised by Fay Elhassan and Karian For</p> <p>MOOVE (Massive Open Online Validation and Evaluation) is a large-scale, participatory evaluation platform designed to collect, structure, and analyze expert feedback on the outputs of clinical large language models (LLMs). Built in collaboration with clinicians and healthcare institutions across diverse geographies including Sub-Saharan Africa, South Asia, Latin America, and Europe MOOVE is the first multilingual, context-sensitive evaluation environment tailored to healthcare AI systems in low- and middle-income as well as high-resource settings.</p>"},{"location":"clusters/cscs/axolotl_training/","title":"Axolotl Training on the CSCS","text":"<p>This tutorial describes how to launch training with axolotl. Axolotl is the pipeline that we use to finetune LLMs and VLMs.</p> <p>This tutorial assumes that:</p> <ol> <li>You have done the setup to connect to the CSCS</li> <li>You have a working axolotl docker image with the correct version of transformers. If you wish to build an image with updated version, please check this; in this tutorial, we provide a Dockerfile to build axolotl with the latest version on GitHub.</li> </ol>"},{"location":"clusters/cscs/axolotl_training/#what-is-axolotl","title":"What is axolotl?","text":"<p>axolotl is a training pipeline that makes training and fine-tuning easier. axolotl configures training using a YAML file that provides the training arguments, datasets and hyperparameters.</p> <p>As a user, you need to do 2 things:</p> <ol> <li>Put your datasets in a compatible format</li> <li>Configure your training</li> </ol>"},{"location":"clusters/cscs/axolotl_training/#setup","title":"Setup","text":"<p>Create a TOML file to define the axolotl environment. On the CSCS, create a <code>~/.edf/axolotl.toml</code>:</p> <pre><code># Put this or replace it with an updated axolotl image\nimage = \"/capstor/store/cscs/swissai/a127/meditron/docker/axolotl.sqsh\"\n\nmounts = [\"/capstor\", \"/iopsstor\", \"/users\"]\n\nwritable = true\n\n[annotations]\ncom.hooks.aws_ofi_nccl.enabled = \"true\"\ncom.hooks.aws_ofi_nccl.variant = \"cuda12\"\n\n[env]\nHF_HOME = \"${SCRATCH}/hf\"\nCUDA_CACHE_DISABLE = \"1\"\nNCCL_NET = \"AWS Libfabric\"\nNCCL_CROSS_NIC = \"1\"\nNCCL_NET_GDR_LEVEL = \"PHB\"\nFI_CXI_DISABLE_HOST_REGISTER = \"1\"\nFI_MR_CACHE_MONITOR = \"userfaultfd\"\nFI_CXI_DEFAULT_CQ_SIZE = \"131072\"\nFI_CXI_DEFAULT_TX_SIZE = \"32768\"\nFI_CXI_RX_MATCH_MODE = \"software\"\nFI_CXI_SAFE_DEVMEM_COPY_THRESHOLD = \"16777216\"\nFI_CXI_COMPAT = \"0\"\n</code></pre>"},{"location":"clusters/cscs/axolotl_training/#formatting-datasets-in-the-right-format","title":"Formatting datasets in the right format","text":"<p>The first step is to convert your dataset into the right format. axolotl supports both datasets for LLMs and VLMs</p>"},{"location":"clusters/cscs/axolotl_training/#text-only-data","title":"Text-only data","text":"<p>For text-only data, axolotl supports both pre-training and conversational data. We recommend storing the dataset in a JSONL format where each line is a sample.</p> <p>Pre-training data: Pre-training samples must be in this format:</p> <pre><code>{ \"text\" : \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\" }\n</code></pre> <p>Pre-training data can be used when:</p> <ul> <li>You have big chunks of documents that describes the knowledge that you want to train on</li> <li>You have \"low-quality\" documents but a lot of them</li> </ul> <p>Conversational data: Conversational samples must be in this format:</p> <pre><code>{\n    \"messages\": [\n        { \"role\": \"user\", \"content\": \"What is the capital of Switzerland?\" } \n        { \"role\": \"assistant\", \"content\": \"The capital of Switzerland is Bern\" }\n    ]\n}\n</code></pre> <p>The <code>\"messages\"</code>, <code>\"role\"</code> and <code>\"content\"</code> are flexible and can be replaced by something else. For instance, you can also use the following format:</p> <pre><code>{\n    \"conversations\": [\n        { \"from\": \"user\", \"value\": \"What is the capital of Switzerland?\" } \n        { \"from\": \"assistant\", \"value\": \"The capital of Switzerland is Bern\" }\n    ]\n}\n</code></pre> <p>Then in the axolotl configuration file, you will have to specify those fields correctly so that your dataset gets tokenized correctly.</p>"},{"location":"clusters/cscs/axolotl_training/#multimodal-data","title":"Multimodal data","text":"<p>TODO!</p>"},{"location":"clusters/cscs/axolotl_training/#configure-your-training","title":"Configure your training","text":"<p>The second step is to create a YAML configuration file that describes your training. </p> <p>Create a folder in your home directory to store your axolotl configurations:</p> <pre><code>mkdir -p ~/meditron/axolotl_config\ncd ~/meditron/axolotl_config\n</code></pre> <p>Here is an example configuration file for fine-tuning the Apertus model. Store this file as <code>axolotl_apertus_8b.yaml</code>:</p> <pre><code>base_model: swiss-ai/Apertus-8B-Instruct-2509\n\ndatasets:\n  - path: /capstor/store/cscs/swissai/a127/path/to/conversations.jsonl\n    type: chat_template\n    split: train\n    field_messages: conversations\n    message_property_mappings:\n      role: from\n      content: value\n  - path: /capstor/store/cscs/swissai/a127/path/to/pretrain.jsonl\n    ds_type: json\n    type: completion\n    field: text\n\n\n# This is the path where axolotl caches the prepared dataset\ndataset_prepared_path: /capstor/store/cscs/swissai/a127/homes/$USER/axolotl_datasets/last_run_prepared\n\n# Output directory where model checkpoints and logs will be saved\noutput_dir: /capstor/store/cscs/swissai/a127/meditron/models/tutorials/axolotl_apertus_8b\n\n# Data loading and processing settings\nshuffle_merged_datasets: true\ndataset_processes: 64 # Avoid RAM OOM issues by lowering this value if needed\n\n# If your model supports flash attention, enable it\nflash_attention: true\nflash_attn_rms_norm: true\nflash_attn_fuse_qkv: false\n\n# Enable/Disable sample packing\nsample_packing: true\nsequence_len: 2048\ngroup_by_length: false\npad_to_sequence_len: true\n\n# Gradient checkpointing settings: enable to save VRAM\ngradient_checkpointing: true\ngradient_checkpointing_kwargs:\n  use_reentrant: false\n\n# Control batch size and number of epochs\ngradient_accumulation_steps: 2\nmicro_batch_size: 8\nnum_epochs: 1\n\n# Learning rate scheduler and optimizer settings\noptimizer: adamw_torch\noptim_args:\n  fused: true\nlearning_rate: 1.0e-5\nwarmup_ratio: 0.0\nweight_decay: 0.05\nlr_scheduler: cosine\ncosine_min_lr_ratio: 0.1\nmax_grad_norm: 1.0\n\n# Disable evaluation\nevals_per_epoch: 0\neval_set_size: 0.0\neval_table_size: null\n\n# Checkpointing and logging settings\nresume_from_checkpoint: null\nlogging_steps: 1\nsaves_per_epoch: 2\n\n# Model and tokenizer types (usually AutoModelForCausalLM and AutoTokenizer for causal LLMs)\ntokenizer_type: AutoTokenizer\ntype: AutoModelForCausalLM\n\n# Weights &amp; Biases logging configuration\nwandb_entity: \nwandb_log_model: \nwandb_name: Meditron-Apertus-8B\nwandb_project: tutorial\nwandb_watch: null\n\nddp_find_unused_parameters: true\ndeepspeed: /users/$USER/meditron/axolotl_config/deepspeed.json\n</code></pre> <p>Replace the <code>$USER</code> by your username in this example configuration.</p> <p>In this example, we only use a conversational dataset and a pretrain dataset for training.</p> <p>For the conversational dataset, we specify that the messages are stored in the <code>conversations</code> field, and that the <code>from</code> and <code>value</code> fields correspond to the role and content of each message respectively. Here is an example of how the data should look like:</p> <pre><code>{\n  \"conversations\": [\n    {\n      \"from\": \"system\",\n      \"value\": \"Answer this question truthfully\"\n    },\n    {\n      \"from\": \"user\",\n      \"value\": \"What is the capital of Switzerland?\"\n    },\n    {\n      \"from\": \"assistant\",\n      \"value\": \"The capital of Switzerland is Bern\"\n    }\n  ]\n}\n</code></pre> <p>The pretraining dataset is simply a JSONL file where each line contains a <code>text</code> field with the document to pretrain on:</p> <pre><code>{ \"text\" : \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\" }\n</code></pre> <p>To enable Deepspeed Zero-3 optimization, create a <code>deepspeed.json</code> file in the same folder with the following content:</p> <pre><code>{\n    \"bf16\": {\n        \"enabled\": true\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n          \"device\": \"cpu\",\n          \"pin_memory\": true\n        },\n        \"overlap_comm\": false,\n        \"contiguous_gradients\": true,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"sub_group_size\": 1e9,\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    },\n    \"gradient_accumulation_steps\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"gradient_clipping\": 1.0,\n    \"wall_clock_breakdown\": false,\n    \"activation_checkpointing\": {\n        \"partition_activations\": false,\n        \"contiguous_memory_optimization\": false,\n        \"cpu_checkpointing\": false\n    },\n    \"flops_profiler\": {\n        \"enabled\": false\n    },\n    \"aio\": {\n        \"block_size\": 1048576,\n        \"queue_depth\": 8,\n        \"single_submit\": false,\n        \"overlap_events\": false\n    }\n}\n</code></pre>"},{"location":"clusters/cscs/axolotl_training/#launch-training-on-the-cscs","title":"Launch training on the CSCS","text":"<p>Now that you have your dataset and configuration file ready, you can launch the training on the CSCS.</p>"},{"location":"clusters/cscs/axolotl_training/#testing-your-configuration-interactively","title":"Testing your configuration interactively","text":"<p>First launch a job in interactive mode to test your configuration:</p> <pre><code>srun --time=2:59:59 --partition normal -A a127 --environment=/users/$USER/.edf/axolotl.toml \n</code></pre> <p>This will launch an interactive session with access to 4 GPUs. Inside the session, launch the training with:</p> <pre><code>torchrun --nproc_per_node=4 -m axolotl.cli.train /users/$USER/meditron/axolotl_config/axolotl_apertus_8b.yaml\n</code></pre> <p>Once you see something similar to the following, your training is running correctly: <pre><code>{ \"loss\" : 2.34567, \"epoch\": 0.01, \"step\": 10, ... }\n</code></pre> The logging above will be printed every <code>logging_steps</code> that you defined in the YAML configuration file.</p> <p>When you are satisfied with your configuration, exit your interactive session with: <pre><code>exit\n</code></pre></p>"},{"location":"clusters/cscs/axolotl_training/#launching-a-batch-job","title":"Launching a batch job","text":"<p>Now, you can create a batch job to launch the training. Create a SLURM script <code>launch_axolotl_apertus_8b.sh</code> with the following content:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name meditron-tutorial\n#SBATCH --chdir /users/$USER/meditron/axolotl_config\n#SBATCH --output /users/$USER/meditron/reports/R-%x.%j.out\n#SBATCH --error /users/$USER/meditron/reports/R-%x.%j.err\n#SBATCH --nodes 4               # number of Nodes\n#SBATCH --ntasks-per-node 1     # number of MP tasks. IMPORTANT: torchrun represents just 1 Slurm task\n#SBATCH --gres gpu:4        # Number of GPUs\n#SBATCH --cpus-per-task 288     # number of CPUs per task (based on lscpu)\n#SBATCH --time 11:59:59       # maximum execution time (DD-HH:MM:SS)\n#SBATCH -A a127\n\nexport WANDB_DIR=/capstor/store/cscs/swissai/a127/homes/$USER/wandb\nexport WANDB_API_KEY=&lt;your_wandb_api_key&gt;\nexport WANDB_MODE=\"online\"\n\n# Put Triton on a non-NFS directory\nexport TRITON_CACHE_DIR=/tmp/$USER/triton_cache\n\nexport CUDA_LAUNCH_BLOCKING=1\necho \"START TIME: $(date)\"\n# auto-fail on any errors in this script\nset -eo pipefail\n# logging script's variables/commands for future debug needs\nset -x\n######################\n### Set enviroment ###\n######################\nGPUS_PER_NODE=4\n\necho \"NODES: $SLURM_NNODES\"\n######## Args ########\nAXOLOTL_CONFIG_FILE=/users/$USER/meditron/axolotl_config/axolotl_apertus_8b.yaml\n\nexport HF_HOME=$SCRATCH/hf\nexport HF_TOKEN=&lt;your_huggingface_token&gt;\nmkdir -p $HF_HOME\n\n######################\n######################\n#### Set network #####\n######################\nMASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\nMASTER_PORT=6300\n######################\n# note that we don't want to interpolate `\\$SLURM_PROCID` till `srun` since otherwise all nodes will get\n# 0 and the launcher will hang\n#\n# same goes for `\\$(hostname -s|tr -dc '0-9')` - we want it to interpolate at `srun` time\n\n\nLAUNCHER=\"\n    torchrun \\\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $SLURM_NNODES \\\n    --node_rank \\$SLURM_PROCID \\\n    --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \\\n    --rdzv_backend c10d \\\n    --max_restarts 0 \\\n    --tee 3 \\\n    \"\n\nexport CMD=\"$LAUNCHER -m axolotl.cli.train $AXOLOTL_CONFIG_FILE\"\necho $CMD\n# srun error handling:\n# --wait=60: wait 60 sec after the first task terminates before terminating all remaining tasks\nSRUN_ARGS=\" \\\n    --cpus-per-task $SLURM_CPUS_PER_TASK \\\n    --jobid $SLURM_JOB_ID \\\n    --wait 60 \\\n    -A a127 \\\n    --environment /users/$USER/.edf/axolotl.toml \\\n    \"\n# bash -c is needed for the delayed interpolation of env vars to work\n\nsrun $SRUN_ARGS bash -c \"$CMD\"\necho \"END TIME: $(date)\"\n</code></pre> <p>Make sure to replace the <code>$USER</code> with your username, and set your Weights &amp; Biases and Hugging Face tokens.</p> <p>You can increase the number of nodes by changing the <code>#SBATCH --nodes</code> parameter according to your needs.</p> <p>Finally, launch the job with:</p> <pre><code>sbatch launch_axolotl_apertus_8b.sh\n</code></pre> <p>You can monitor the status of your job with:</p> <pre><code>squeue -u $USER\n</code></pre> <p>To get the logs of your job, check the files in the <code>~/meditron/reports/</code> folder. <pre><code>cd ~/meditron/reports/\ntail -f R-meditron-tutorial.&lt;job_id&gt;.err\n</code></pre></p> <p>That's it! You have successfully launched an axolotl training job on the CSCS.</p>"},{"location":"clusters/cscs/axolotl_training/#further-reading","title":"Further reading","text":"<ul> <li>Axolotl documentation</li> <li>CSCS documentation</li> </ul>"},{"location":"clusters/cscs/cscs/","title":"Connecting to CSCS","text":"<p>DO NOT RUN ON LOGIN NODE</p> <p>When you establish a direct connection using <code>ssh</code> you connect to the login node. Everyone is on that node and as such YOU SHOULD NEVER RUN ANY</p> <p>JOBS DIRECTLY ON THE LOGIN NODE. If you want to run a process, like a training, you can run it on a dedicated allocated job</p>"},{"location":"clusters/cscs/cscs/#pre-setup-access-to-the-cscs","title":"Pre-setup (access to the CSCS)","text":"<p>Please ask Peter to add you to the CSCS project (send a message on Slack to get a faster answer). Once you have been added, check your mail for the invitation link. You will to have to create an account.</p>"},{"location":"clusters/cscs/cscs/#connect-to-the-login-node","title":"Connect to the login node","text":"<p>To connect to the login node, you will need to refresh your key every 24 hours. To refresh your keys, you need to execute the following script. Store the following script in a <code>.sh</code> file (e.g. <code>cscs_connect.sh</code>). Make sure to replace <code>$CSCS_USER</code> with your CSCS username and the <code>$CSCS_PASSWORD</code> with your CSCS password.</p> <pre><code>#!/bin/bash\n\n# This script sets the environment properly so that a user can access CSCS\n# login nodes via ssh. \n\n#    Copyright (C) 2023, ETH Zuerich, Switzerland\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, version 3 of the License.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.\n#\n#    AUTHORS Massimo Benini\n\n\nUSERNAME=$CSCS_USER\nPASSWORD=$CSCS_PASSWORD\n#read -p \"Username : \" USERNAME\n#read -s -p \"Password: \" PASSWORD\n\nfunction ProgressBar {\n# Process data\n    let _progress=(${1}*100/${2}*100)/100\n    let _done=(${_progress}*4)/10\n    let _left=40-$_done\n# Build progressbar string lengths\n    _fill=$(printf \"%${_done}s\")\n    _empty=$(printf \"%${_left}s\")\n\n# 1.2 Build progressbar strings and print the ProgressBar line\n# 1.2.1 Output example:\n# 1.2.1.1 Progress : [########################################] 100%\nprintf \"\\rSetting the environment : [${_fill// /#}${_empty// /-}] ${_progress}%%\"\n}\n\n#Variables\n_start=1\n#This accounts as the \"totalState\" variable for the ProgressBar function\n_end=100\n\n#Params\nMFA_KEYS_URL=\"https://sshservice.cscs.ch/api/v1/auth/ssh-keys/signed-key\"\n\n#Detect OS\nOS=\"$(uname)\"\ncase \"${OS}\" in\n  'Linux')\n    OS='Linux'\n    ;;\n  'FreeBSD')\n    OS='FreeBSD'\n    ;;\n  'WindowsNT')\n    OS='Windows'\n    ;;\n  'Darwin')\n    OS='Mac'\n    ;;\n  *) ;;\nesac\n\n#OS validation\nif [ \"${OS}\" != \"Mac\" ] &amp;&amp; [ \"${OS}\" != \"Linux\" ]; then\n  echo \"This script works only on Mac-OS or Linux. Abording.\"\n  exit 1\nfi\n\n#Read Inputs\necho\nread -s -p \"Enter OTP (6-digit code): \" OTP\necho\n\nif [ -z \"${PASSWORD}\" ]; then\n    echo \"Password is empty.\"\n    exit 1\nfi\n\nif ! [[ \"${OTP}\" =~ ^[[:digit:]]{6} ]]; then\n    echo \"OTP is not valid, OTP must contains only six digits.\"\n    exit 1\nfi\n\nProgressBar 25 \"${_end}\"\necho \"  Authenticating to the SSH key service...\"\n\nHEADERS=(-H \"Content-Type: application/json\" -H \"accept: application/json\")\nKEYS=$(curl -s -S --ssl-reqd \\\n    \"${HEADERS[@]}\" \\\n    -d \"{\\\"username\\\": \\\"$USERNAME\\\", \\\"password\\\": \\\"$PASSWORD\\\", \\\"otp\\\": \\\"$OTP\\\"}\" \\\n    \"$MFA_KEYS_URL\")\n\nif [ $? != 0 ]; then\n    exit 1\nfi\n\nProgressBar 50 \"${_end}\"\necho \"  Retrieving the SSH keys...\"\n\nDICT_KEY=$(echo ${KEYS} | cut -d \\\" -f 2)\nif [ \"${DICT_KEY}\" == \"payload\" ]; then\n   MESSAGE=$(echo ${KEYS} | cut -d \\\" -f 6)\n   ! [ -z \"${MESSAGE}\" ] &amp;&amp; echo \"${MESSAGE}\"\n   echo \"Error fetching the SSH keys. Aborting.\"\n   exit 1\nfi\n\nPUBLIC=$(echo ${KEYS} | cut -d \\\" -f 4)\nPRIVATE=$(echo ${KEYS} | cut -d \\\" -f 8)\n\n#Check if keys are empty:\nif [ -z \"${PUBLIC}\" ] || [ -z \"${PRIVATE}\" ]; then\n    echo \"Error fetching the SSH keys. Aborting.\"\n    exit 1\nfi\n\nProgressBar 75 \"${_end}\"\necho \"  Setting up the SSH keys into your home folder...\"\n\n#Check ~/.ssh folder and store the keys\necho ${PUBLIC} | awk '{gsub(/\\\\n/,\"\\n\")}1' &gt; ~/.ssh/cscs-key-cert.pub || exit 1\necho ${PRIVATE} | awk '{gsub(/\\\\n/,\"\\n\")}1' &gt; ~/.ssh/cscs-key || exit 1\n\n#Setting permissions:\nchmod 644 ~/.ssh/cscs-key-cert.pub || exit 1\nchmod 600 ~/.ssh/cscs-key || exit 1\n\n#Format the keys:\nif [ \"${OS}\" = \"Mac\" ]\nthen\n  sed -i '' -e '$ d' ~/.ssh/cscs-key-cert.pub || exit 1\n  sed -i '' -e '$ d' ~/.ssh/cscs-key || exit 1\nelse [ \"${OS}\" = \"Linux\" ]\n  sed '$d' ~/.ssh/cscs-key-cert.pub || exit 1\n  sed '$d' ~/.ssh/cscs-key || exit 1\nfi\n\nProgressBar 100 \"${_end}\"\necho \"  Completed.\"\n\nexit_code_passphrase=1\nread -n 1 -p \"Do you want to add a passphrase to your key? [y/n] (Default y) \" reply; \nif [ \"$reply\" != \"\" ];\n then echo;\nfi\nif [ \"$reply\" = \"${reply#[Nn]}\" ]; then\n      while [ $exit_code_passphrase != 0 ]; do\n        ssh-keygen -f ~/.ssh/cscs-key -p\n        exit_code_passphrase=$?\n      done\nfi\n\nif (( $exit_code_passphrase == 0 ));\n  then\n    SUBSTRING=\", using the passphrase you have set:\";\n  else\n     SUBSTRING=\":\";\nfi     \n\neval `ssh-agent -s`\nssh-add -t 1d ~/.ssh/cscs-key\n</code></pre> <p>You will have to execute this bash script every day. Warning, you are limited in the number of key you can generate by day (roughtly 5 per day) as such be mindful when trying to debug things. You should launch this script in <code>bash</code>, you can run the following commands</p> <pre><code>nano cscs-refresh.sh # update the file\nchmod +x ./cscs-refresh.sh\nbash ./cscs-refresh.sh\n</code></pre> <p>If you don't want to have your login ID stored in a script, you can comment out the lines:</p> <pre><code>#read -p \"Username : \" USERNAME\n#read -s -p \"Password: \" PASSWORD\n</code></pre> <p>and remove the lines:</p> <pre><code>USERNAME=$CSCS_USER\nPASSWORD=$CSCS_PASSWORD\n</code></pre>"},{"location":"clusters/cscs/cscs/#setup-your-ssh-config","title":"Setup your ssh config","text":"<p>Add the following lines to the <code>~/.ssh/config</code> file:</p> <pre><code>Host ela\n    HostName ela.cscs.ch\n    User $CSCS_USER\n    ForwardAgent yes\n    ForwardX11 yes\n    forwardX11Trusted yes\n IdentityFile ~/.ssh/cscs-key\n\n\nHost todi\n    HostName todi.cscs.ch\n    User $CSCS_USER\n    ProxyJump ela\n    ForwardAgent yes\n    ForwardX11 yes\n    forwardX11Trusted yes\n IdentityFile ~/.ssh/cscs-key\n\nHost clariden\n    HostName clariden.cscs.ch\n    User $CSCS_USER\n    ProxyJump ela\n    ForwardAgent yes\n    ForwardX11 yes\n    forwardX11Trusted yes\n    IdentityFile ~/.ssh/cscs-key\n</code></pre> <p>Make sure to replace the <code>$CSCS_USER</code> by your real CSCS username.</p> <p>To connect to the cluster, run the following:</p> <pre><code># Your terminal\n\nssh clariden\n</code></pre> <p>DO NOT RUN ON LOGIN NODE</p> <p>This opens a terminal on the CSCS login node. You should have a terminal that looks like this: <pre><code>[mzhang@clariden-lnxxx ~]$\n</code></pre></p> <p>DO NOT RUN ANYTHING HEAVY WHEN YOU SEE THIS PROMPT (training, inference, heavy download, etc.). You should always ask for a job first. Here <code>ln</code> stands for login node, this is how you know that you are on the login node. To launch a job, see how to use an environment and how to launch a job</p>"},{"location":"clusters/cscs/cscs/#setup-github","title":"Setup Github","text":"<p>To operate on private repositories on GitHub. You can either generate a SSH key pairs or use a GitHub personal access token (GitHub PAT). We recommand doing the second option but both options are viable.</p> <p>To generate a GitHub PAT, follow those instructions. Make sure that this PAT is stored somewhere</p> <p>For this tutorial, we are gonna use the <code>MultiMeditron</code> training pipeline setup. Clone the MultiMeditron repository in your user directory:</p> HTTP cloneSSH clone <pre><code># CSCS login node\n\nmkdir /users/$USER/meditron\ncd /users/$USER/meditron\n\ngit clone https://github.com/EPFLiGHT/MultiMeditron.git\n</code></pre> <pre><code># CSCS login node\n\nmkdir /users/$USER/meditron\ncd /users/$USER/meditron\n\ngit clone git@github.com:EPFLiGHT/MultiMeditron.git\n</code></pre> <p>When GitHub asks for your password, input the PAT that you have generated in this step.</p>"},{"location":"clusters/cscs/cscs/#create-a-personal-folder-in-the-capstor-partition","title":"Create a personal folder in the capstor partition","text":"<pre><code># CSCS login node\n\nmkdir /capstor/store/cscs/swissai/a127/homes/$USER\n</code></pre> <p>This personal folder on the capstor will be mainly used to store your huggingface home and your big files that don't fit in your users personal folder</p>"},{"location":"clusters/cscs/cscs/#setup-the-environment-on-the-cluster","title":"Setup the environment on the cluster","text":"<p>The terminal will spawn you into the <code>/users/$USER</code> directory.</p> <p>When running job, you will need to execute your job inside docker images. This is done by using <code>.toml</code> files that specify which docker image, environment variables are gonne be set when running the job. Create a folder <code>.edf</code> in <code>/users/$USER</code>:</p> <pre><code># CSCS login node\n\nmkdir /users/$USER/.edf\n</code></pre> <p>Create a <code>/users/$USER/.edf/multimodal.toml</code> file:</p> <pre><code>image = \"michelducartier24/multimeditron-git:latest-arm64\"\nmounts = [\"/capstor\", \"/iopsstor\", \"/users\"]\n\nwritable = true\n\nworkdir = \"/users/${USER}/meditron/MultiMeditron\"\n\n[annotations]\ncom.hooks.aws_ofi_nccl.enabled = \"true\"\ncom.hooks.aws_ofi_nccl.variant = \"cuda12\"\n\n[env]\nHF_HOME = \"${SCRATCH}/hf\"\nCUDA_CACHE_DISABLE = \"1\"\nNCCL_NET = \"AWS Libfabric\"\nNCCL_CROSS_NIC = \"1\"\nNCCL_NET_GDR_LEVEL = \"PHB\"\nFI_CXI_DISABLE_HOST_REGISTER = \"1\"\nFI_MR_CACHE_MONITOR = \"userfaultfd\"\nFI_CXI_DEFAULT_CQ_SIZE = \"131072\"\nFI_CXI_DEFAULT_TX_SIZE = \"32768\"\nFI_CXI_RX_MATCH_MODE = \"software\"\nFI_CXI_SAFE_DEVMEM_COPY_THRESHOLD = \"16777216\"\nFI_CXI_COMPAT = \"0\"\n</code></pre> <p>Notice 3 things:</p> <ul> <li>We specify the path to the <code>.sqsh</code> file in the <code>image</code> attribute. This is the image used by the job that stores all of the dependencies.</li> <li>We specify the path to the MultiMeditron repo in the <code>workdir</code> attribute. This is the directory where we spawn when the job is launched.</li> <li>We specify the path to the HF cache in the environment variable <code>$HF_HOME</code></li> </ul> <p>Note that for other types of job, you will probably require a different image and a different working directory.</p>"},{"location":"clusters/cscs/cscs/#launching-job","title":"Launching job","text":"<p>There are 2 types of job that you can launch:</p> <ul> <li>Interactive using <code>srun</code> (which gives you a terminal)</li> <li>Non-interactive using <code>sbatch</code> (which schedule a job)</li> </ul>"},{"location":"clusters/cscs/cscs/#interactive-job","title":"Interactive job","text":"<p>On the login node, you can launch an interactive job by executing the following command:</p> <pre><code>srun --time=1:29:59 --partition debug -A a127 --environment=/users/$USER/.edf/multimodal.toml --pty bash\n</code></pre> <p>Here is a breakdown of the command:</p> <ul> <li><code>--time</code> is the maximum running time of the job (here, the job runs for 1h30 before it gets killed)</li> <li> <p><code>--partition debug</code> is the node partition in which the job executed. As of 14/08/2025, there are 3 partitions:</p> </li> <li> <p><code>normal</code>: with a maximum running time of 12 hours and no limit on the number of distributed nodes. This partition is the partition used for non-interactive jobs and long interactive jobs</p> </li> <li><code>debug</code>: with a maximum running time of 1h30 with only one node. This partition is meant for interactive jobs</li> <li><code>xfer</code>: this partition is meant for data transfer and doesn't claim any GPU</li> </ul> <p>To check if you have been allocated a node, run the following command in another terminal:</p> <pre><code># CSCS login node\n\nsqueue --me --start\n</code></pre> <p>This command will give you a dynamic estimation of the scheduled time (may change as people pass you in the priority queue). Note that this command doesn't output anything if your job has been allocated.</p> <p>Once you have been allocated a job, you will have a terminal inside the allocated node. Make sure that your <code>bash prompt</code> is of the form <code>$USER@nidxxxxxx</code> (and not <code>[clariden][$USER@clariden-lnxxx]</code>.</p> <p>Furthermore:</p> <pre><code>echo $HF_HOME\n</code></pre> <p>Make sure that the output is <code>/iopsstor/scratch/cscs/$USER/hf</code>. This is extremely important because if you run trainings without telling it where to download the Llama-3.1 model, it will do so in your working directory <code>/users/$USER</code> and you do not have enough storage for that.</p> <p>Launch a training with MultiMeditron by running the following commands:</p> <pre><code>cd MultiMeditron\nexport \npip install -e .\ntorchrun --nproc-per-node 4 train.py --config config/config_alignment.yaml\n</code></pre> <p>Make sure Llama 3.1 is not gated.</p> <p>Once you are done with the job. Type <code>exit</code> to exit the terminal to exit the terminal. This will cancel your job.</p>"},{"location":"clusters/cscs/cscs/#non-interactive-job","title":"Non-interactive job","text":"<p>To launch a non-interactive job, you need to create a sbatch script. Create a file called <code>sbatch_train.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name demo-job\n#SBATCH --output /users/$USER/meditron/reports/R-%x.%j.out\n#SBATCH --error /users/$USER/meditron/reports/R-%x.%j.err\n#SBATCH --nodes 1         # number of Nodes\n#SBATCH --ntasks-per-node 1     # number of MP tasks. IMPORTANT: torchrun represents just 1 Slurm task\n#SBATCH --gres gpu:4        # Number of GPUs\n#SBATCH --cpus-per-task 288     # number of CPUs per task.\n#SBATCH --time 0:59:59       # maximum execution time (DD-HH:MM:SS)\n#SBATCH -A a127\n\nexport WANDB_DIR=/capstor/store/cscs/swissai/a127/homes/$USER/wandb\nexport WANDB_MODE=\"offline\"\nexport HF_TOKEN=$HF_TOKEN\nexport SETUP=\"cd /users/$USER/meditron/multimodal/MultiMeditron &amp;&amp; pip install -e .\"\n\nexport CUDA_LAUNCH_BLOCKING=1\necho \"START TIME: $(date)\"\n# auto-fail on any errors in this script\nset -eo pipefail\n# logging script's variables/commands for future debug needs\nset -x\n######################\n### Set enviroment ###\n######################\nGPUS_PER_NODE=4\necho \"NODES: $SLURM_NNODES\"\n\n######## Args ########\nexport HF_HOME=$SCRATCH/hf\n\n######################\n######################\n#### Set network #####\n######################\nMASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\nMASTER_PORT=6200\n######################\n# note that we don't want to interpolate `\\$SLURM_PROCID` till `srun` since otherwise all nodes will get\n# 0 and the launcher will hang\n#\n# same goes for `\\$(hostname -s|tr -dc '0-9')` - we want it to interpolate at `srun` time\n\nLAUNCHER=\"\n  torchrun \\\n  --nproc_per_node $GPUS_PER_NODE \\\n  --nnodes $SLURM_NNODES \\\n  --node_rank \\$SLURM_PROCID \\\n  --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \\\n  --rdzv_backend c10d \\\n  --max_restarts 0 \\\n  --tee 3 \\\n  \"\n\nexport CMD=\"$LAUNCHER -m multimeditron train /users/$USER/meditron/MultiMeditron/config/config_alignment.yaml\"\n\necho $CMD\n\n# srun error handling:\n# --wait=60: wait 60 sec after the first task terminates before terminating all remaining tasks\nSRUN_ARGS=\" \\\n  --cpus-per-task $SLURM_CPUS_PER_TASK \\\n  --jobid $SLURM_JOB_ID \\\n  --wait 60 \\\n  -A a127 \\\n  --reservation=sai-a127 \\\n  --environment /users/$USER/.edf/multimodal.toml\n  \"\n# bash -c is needed for the delayed interpolation of env vars to work\nsrun $SRUN_ARGS bash -c \"$SETUP &amp;&amp; $CMD\"\necho \"END TIME: $(date)\"\n</code></pre> <p>Make sure to replace all the <code>$USER</code> by your username and the <code>$HF_TOKEN</code> with your huggingface token. Pay attention to the following parameters:</p> <ul> <li><code>#SBATCH --job-name demo-job</code> sets the job name to <code>demo-job</code></li> <li><code>#SBATCH --nodes 1</code> means that we are claiming one node (of 4 GPUs). You should increase this if you are launching bigger jobs</li> <li><code>#SBATCH --output /users/$USER/meditron/reports/R-%x.%j.out</code> and <code>#SBATCH --error /users/$USER/meditron/reports/R-%x.%j.err</code> mean that this will create a folder <code>/users/$USER/meditron/reports</code> that stores all the job logs</li> <li>Note that here, we execute a training of MultiMeditron with <code>config/config_alignment.yaml</code>, thus you need to make sure that the paths of the dataset are correct</li> <li>Note that the part which follows the <code>#SBATCH</code> commands will be executed on every node</li> </ul> <p>To queue your job, run: bash</p> <pre><code># CSCS login node\n\nsbatch sbatch_train.sh\n</code></pre> <p>You can check if your job has been allocated GPUs by running:</p> <pre><code># CSCS login node\n\nsqueue --me\n</code></pre> <p>This command gives you the <code>JOBID</code> of the job you have launched</p> <p>Once the job enters the <code>R</code> state (for running), the job is running. You can check the logs of your job by going into the <code>reports</code> directory:</p> <pre><code># Login node\n\ncd /users/$USER/meditron/reports/\ntail -f R-%x.%j.err\n</code></pre> <p>where you need to replace <code>R-%x.%j.err</code> by the actual report name.</p> <p>You can either let the job finishes or cancels the job.</p> <pre><code># Login node\n\nscancel $JOBID\n</code></pre> <p>where <code>$JOBID</code>is the <code>JOBID</code> that you get when running <code>squeue --me</code></p>"},{"location":"clusters/cscs/cscs/#vscode-connection","title":"VSCode Connection","text":"<p>If you want to join the modern era of computers and have something more involve than a terminal to code (unlike some people), you may want to \"connect\" your visual studio code instance directly to the cluster. This allows to directly modify the code, using the correct environment (so that it doesn't show you half the package as non existent).</p>"},{"location":"clusters/cscs/cscs/#procedure","title":"Procedure","text":"<ul> <li>Install the Remote development extension</li> <li>Launch a job on the cluster</li> </ul> <p>You will need the vscode CLI installed on the job you launched.</p> Use prebuilt imageManually install CLI <p>You can use the image that I personally used, you can update your environment file, and use the image at <code>/capstor/store/cscs/swissai/a127/meditron/docker/multimeditron_latest_2.sqsh</code>. With this solution however you'll inherit from all of my python dependencies. If you want to use your own image, you can check the manual installation.</p> <p>If you want to use custom dependency, you'll need to manually install the vscode cli onto you image. To show you an example of it, here's a sample of my <code>Dockerfile</code> responsible for installing the CLI.</p> Sample Dockerfile<pre><code>FROM michelducartier24/multimeditron-apertus\nRUN pip install -U transformers\n\nRUN echo \"\" &gt; /etc/pip/constraint.txt\n\nRUN mkdir -p /workspace/code\nWORKDIR /workspace/code\nRUN curl -Lk 'https://code.visualstudio.com/sha/download?build=stable&amp;os=cli-alpine-arm64' --output vscode_cli.tar.gz\nRUN tar -xf vscode_cli.tar.gz\nRUN mv ./code /usr/bin\nRUN rm -rf /workspace/code\n</code></pre> <ul> <li>Once your job has been launched with vscode CLI installed, it's time to run the code tunnel within the job. Go to the folder of your project and run the following command</li> </ul> <pre><code>cd /path/to/my/awesome/project\ncode tunnel --name=cluster-tunnel\n</code></pre> <p>This will prompt you to connect to your <code>github</code> account, do so.</p> <p>Bug in CSCS after update</p> <p>After previous maintainance of CSCS there was a bug where the following code no longer worked. This was due to multiple proxy variable being set. To fix that bug please use the following code:</p> <pre><code>unset {http,https,no}_proxy\nunset {HTTP,HTTPS,NO}_PROXY\n</code></pre> <ul> <li>Finally, open vscode locally on your computer then in the remote extension select the appropriate tunnel and that's it, you are in !</li> </ul>"},{"location":"clusters/cscs/cscs/#further-reading","title":"Further reading","text":"<ul> <li>Slurm documentation: https://slurm.schedmd.com/overview.html</li> <li>CSCS documentation: https://docs.cscs.ch/</li> <li>Open a ticket on the CSCS: https://jira.cscs.ch/plugins/servlet/desk</li> </ul>"},{"location":"clusters/cscs/cscs_docker/","title":"Building images for the CSCS","text":"<p>This tutorial assumes that you followed the CSCS setup tutorial at cscs.md</p>"},{"location":"clusters/cscs/cscs_docker/#claim-a-job-to-build-docker-image","title":"Claim a job to build docker image","text":"<p>Connect to the CSCS login node:</p> <pre><code>ssh clariden\n</code></pre> <p>Claim a job:</p> <pre><code># On the login node\n\nsrun --time 11:59:59 -p normal -A a127 --pty bash\n</code></pre> <p>Make sure to put a big timeout as building docker images can take a lot of time</p>"},{"location":"clusters/cscs/cscs_docker/#configure-podman-storage","title":"Configure podman storage","text":"<p>In order to use podman on alps, you need to create a valid container storage configuration file at <code>$HOME/.config/containers/storage.conf</code>. In pratice you need to crete the following file at <code>/users/&lt;USERNAME&gt;/.config/containers/storage.conf</code>. </p> <pre><code>[storage]\ndriver = \"overlay\"\nrunroot = \"/dev/shm/$USER/runroot\"\ngraphroot = \"/dev/shm/$USER/root\"\n\n[storage.options.overlay]\nmount_program = \"/usr/bin/fuse-overlayfs-1.13\"\n</code></pre>"},{"location":"clusters/cscs/cscs_docker/#building-the-image","title":"Building the image","text":"<p>Create your Dockerfile and name it <code>Dockerfile</code>. For example, this is the Dockerfile used to run axolotl:</p> <pre><code>FROM nvcr.io/nvidia/pytorch:24.07-py3\n\n# setup\nRUN apt-get update &amp;&amp; apt-get install python3-pip python3-venv -y\nRUN pip install --upgrade pip setuptools\n\n# Axolotl installs\nRUN MAX_JOBS=24 pip install flash-attn==2.6.2 --no-build-isolation\nRUN pip install trl==0.9.6\nRUN pip install fschat@git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe\nRUN pip install deepspeed==0.14.4\n\nRUN MAX_JOBS=8 TORCH_CUDA_ARCH_LIST=\"9.0\" pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers\nRUN pip install transformers@git+https://github.com/huggingface/transformers.git@026a173a64372e9602a16523b8fae9de4b0ff428\n\nCOPY axolotl/ /workspace/axolotl\nWORKDIR /workspace/axolotl\nRUN pip install -e .\n</code></pre> <p>Note that this Dockerfile assumes that you have the axolotl repository cloned in <code>path/to/root_directory</code></p> <p>Run:</p> <pre><code># Recommended: change the tag from my_awesome_image to a more meaningful one\n\npodman build -f /path/to/root_directory/Dockerfile -t my_awesome_image path/to/root_directory\n</code></pre> <p>You can change the base docker image (the <code>FROM</code> value) to have a more recent version of the CUDA driver. You can check the available tags here.</p> <p>IMPORTANT NOTE: Note that the CSCS cluster uses an ARM64 architecture. Make sure that you are building your packages for this architecture. Some libraries like VLLM may be trickier to install as they rely on low-level optimization.</p>"},{"location":"clusters/cscs/cscs_docker/#export-the-docker-image-to-a-sqsh-file","title":"Export the docker image to a .sqsh file","text":"<p>Run the following command, you can change the <code>my_custom_image.sqsh</code> file name to a more meaningful one.</p> <pre><code>enroot import -o /capstor/store/cscs/swissai/a127/meditron/docker/my_custom_image.sqsh podman://localhost/my_awesome_image:latest\n</code></pre> <p>Set the correct permissions:</p> <pre><code>setfacl -b /capstor/store/cscs/swissai/a127/meditron/docker/my_custom_image.sqsh\nchmod +r /capstor/store/cscs/swissai/a127/meditron/docker/my_custom_image.sqsh\n</code></pre>"},{"location":"clusters/cscs/cscs_docker/#use-your-new-docker-image","title":"Use your new Docker image","text":"<p>To use your new Docker image, create a new toml file in <code>$HOME/.edf</code> (in this example we name it <code>example.toml</code>):</p> <pre><code>image = \"/capstor/store/cscs/swissai/a127/meditron/docker/my_custom_image.sqsh\"\nmounts = [\"/capstor\", \"/iopsstor\", \"/users\"]\n\nwritable = true\n\n# Uncomment this to set a particular working directory\n# workdir = \"/path/to/workdir\"\n\n[annotations]\ncom.hooks.aws_ofi_nccl.enabled = \"true\"\ncom.hooks.aws_ofi_nccl.variant = \"cuda12\"\n\n[env]\nCUDA_CACHE_DISABLE = \"1\"\nNCCL_NET = \"AWS Libfabric\"\nNCCL_CROSS_NIC = \"1\"\nNCCL_NET_GDR_LEVEL = \"PHB\"\nFI_CXI_DISABLE_HOST_REGISTER = \"1\"\nFI_MR_CACHE_MONITOR = \"userfaultfd\"\nFI_CXI_DEFAULT_CQ_SIZE = \"131072\"\nFI_CXI_DEFAULT_TX_SIZE = \"32768\"\nFI_CXI_RX_MATCH_MODE = \"software\"\nFI_CXI_SAFE_DEVMEM_COPY_THRESHOLD = \"16777216\"\nFI_CXI_COMPAT = \"0\"\n</code></pre> <p>Note the line starting with <code>image =</code></p> <p>Then, to claim an interactive job with this image:</p> <pre><code>srun --time=1:29:59 --partition debug -A a127 --environment=$HOME/.edf/example.toml --pty bash\n</code></pre> <p>For non interactive job. Look for the <code>--environment</code> argument in the srun arguments and change it to your desired <code>.toml</code> file (here <code>example.toml</code></p>"},{"location":"clusters/cscs/intro_cscs/","title":"Introduction to the CSCS cluster","text":"<p>The CSCS cluster is a cluster built for Swiss researchers that provides national high-performance computing. This cluster should be used for model training and inference:</p> <ul> <li>To connect to the cluster: see this</li> <li>To build images for the cluster: see this</li> </ul> <p>Here is the overview of the architecture:</p> <pre><code>architecture-beta\n    group api[CSCS cluster]\n    group nodes[GPU nodes] in api\n\n    service laptop(mids:laptop)[Your Laptop]\n    service storage(database)[Storage] in api\n    service login(server)[Login node] in api\n\n    service node1(server)[nid0000] in nodes\n    service node2(server)[nid0001] in nodes\n    service node3(server)[nid0002]in nodes\n    service node4(server)[nid0003]in nodes\n\n    junction junctionStorage in api\n\n    laptop:R --&gt; L:login\n\n    login:B -- T:storage\n    login:R --&gt; L:node1{group}\n\n    junctionStorage:T -- B:node1{group}\n    storage:R -- L:junctionStorage\n\n    node1:T -- B:node2\n    node1:R -- L:node3\n    node2:R -- L:node4\n    node3:T -- B:node4</code></pre>"},{"location":"clusters/rcp/rcp/","title":"Connecting to RCP","text":""},{"location":"clusters/rcp/rcp/#1-pre-setup-access-to-scratch-and-cluster","title":"1. Pre-setup (access to scratch and cluster)","text":"<p>Please ask Peter to add you to the corresponding groups (send a message on Slack to get a faster answer). You can check your groups at groups.epfl.ch.</p>"},{"location":"clusters/rcp/rcp/#2-setting-up-credentials","title":"2. Setting-up credentials","text":"<p>This part makes sure that you have access to GitHub, wandb and huggingface from the cluster. If it's not already done, create an account on those platforms! To setup the credentials, we must access the scratch in <code>haas001.rcp.epfl.ch</code> using ssh. Note that you can also replace the <code>$GASPAR</code> by your EPFL mail address. The password is your GASPAR credentials: <pre><code>ssh $GASPAR@haas001.rcp.epfl.ch\n</code></pre></p> <p>For this part, every command will be done from the ssh terminal</p> <p>Go in the scratch directory (<code>/mnt/light/scratch</code>): <pre><code># SSH terminal\n\ncd /mnt/light/scratch/users\nmkdir $GASPAR\n</code></pre></p>"},{"location":"clusters/rcp/rcp/#wandb-and-huggingface-credentials","title":"WANDB and HuggingFace credentials","text":"<p>We will store the API keys within our directory in a folder that only our user will have access to. Both the wandb and Hugging Face API keys will be stored in a .txt file within this protected folder.</p> <p><pre><code># SSH terminal\n\ncd /mnt/light/scratch/users/$GASPAR_USER\nmkdir keys\ncd keys\ntouch hf_key.txt\ntouch wandb_key.txt\nchmod 700 -R ../keys/\n</code></pre> * For Huggingface: you can create a new access token at https://huggingface.co/settings/tokens. Put this token in the file <code>hf_key.txt</code> * For WANDB: you can access your tokens at https://wandb.ai/settings. Scroll down to \"API keys\". Put this token in the file <code>wandb_key.txt</code></p>"},{"location":"clusters/rcp/rcp/#github-credentials","title":"Github credentials","text":"<p>To carry out the automatic login to GitHub, we will need to store our git identification (.gitconfig) and our access credentials (.git-credentials), which in this case we will do using a Personal Access Token.</p> <p>To do this, we will need to set the environment variable <code>$HOME</code> to the personal folder we have created and activate the credential helper that will store our access credentials. <pre><code># SSH terminal\n\nexport HOME=/mnt/light/scratch/users/$GASPAR_USER\ngit config --global credential.helper store\n</code></pre></p> <p>Then we will configure our git identification, specifying a username and email address. <pre><code># SSH terminal\n\ngit config --global user.name \"GITHUB_USERNAME\"\ngit config --global user.email \"MY_NAME@example.com\"\n</code></pre></p> <p>Create a Personal access token. Select <code>Generate new token</code> and the <code>classic</code> option. Give every permissions to this token.</p> <p>Finally, we will execute an action that requires our identification on GitHub to enter our access credentials and store them (e.g. Clone a private repository). When prompted for the password, we will enter the Personal Access Token that we created: <pre><code># SSH terminal\n\ngit clone https://github.com/some/private_repo.git\n</code></pre> If you were able to clone the repo, then your setup is correct.</p>"},{"location":"clusters/rcp/rcp/#remote-vscode-configuration","title":"Remote VSCode configuration","text":"<p>We will store the configurations related to VSCode in a folder in the scratch volume so that we don't have to download them every time we start a new container. <pre><code># SSH terminal\n\nmkdir /mnt/light/scratch/users/$GASPAR_USER/.vscode-server\n</code></pre></p>"},{"location":"clusters/rcp/rcp/#3-setup-runai-and-kubectl-on-your-machine","title":"3. Setup runai and kubectl on your machine","text":"<p>IMPORTANT: The setup below was tested on macOS with Apple Silicon. If you are using a different system, you may need to adapt the commands. For Windows, we have no experience with the setup and thereby recommend WSL (Windows Subsystem for Linux) to run the commands. If you choose WSL, you should choose the commands as if you were running Linux.</p> <p>Install kubectl</p> <pre><code># Your terminal (either WSL, Linux or Mac)\n\ncurl -LO \"https://dl.k8s.io/release/v1.29.6/bin/linux/amd64/kubectl\" # Linux\n# curl -LO \"https://dl.k8s.io/release/v1.29.6/bin/darwin/arm64/kubectl\" # macOS\n\n# Give it the right permissions and move it.\nchmod +x ./kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl\nsudo chown root: /usr/local/bin/kubectl\n</code></pre> <p>Setup the kube config file: Take our template file kubeconfig.yaml as your config in the home folder ~/.kube/config. Note that the file on your machine has no suffix.</p> <pre><code># Your terminal\n\nmkdir ~/.kube/\ncurl -o  ~/.kube/config https://raw.githubusercontent.com/epfml/getting-started/main/kubeconfig.yaml\n</code></pre> <p>Install the run:ai CLI for RCP (two RCP clusters):</p> <pre><code># Your terminal\n\n# Download the CLI from the link shown in the help section.\n# for macOS: replace `linux` with `darwin`\nwget --content-disposition https://rcp-caas-prod.rcp.epfl.ch/cli/linux\n# Give it the right permissions and move it.\nchmod +x ./runai\nsudo mv ./runai /usr/local/bin/runai\nsudo chown root: /usr/local/bin/runai\n</code></pre>"},{"location":"clusters/rcp/rcp/#4-login","title":"4. Login","text":"<p>The RCP is organized into a 3 level hierarchy. The department is the laboratory (e.g. LiGHT). The projects determine which scratch (aka persistent storage) we have access to. Note that you should choose the SSO option when executing <code>runai login</code>.</p> <pre><code># Your terminal\n\nrunai config cluster rcp-caas-prod\nrunai login\nrunai list project\nrunai config project light-$GASPAR\n</code></pre>"},{"location":"clusters/rcp/rcp/#5-submit-a-job","title":"5. Submit a job","text":"<p>Build your image following the Docker tutorial. Once you are done, it's time to test if we can submit a job! This command will allocate 1 GPU from the cluster and \"sleep\" to infinity (meaning that it will do essentially nothing) </p> <pre><code># Your terminal\n\nrunai submit \\\n  --name base-job \\\n  --image registry.rcp.epfl.ch/multimeditron/basic:latest-$GASPAR\\\n  --pvc light-scratch:/lightscratch \\\n  --large-shm \\\n  -e NAS_HOME=/lightscratch/users/$GASPAR \\\n  -e HF_API_KEY_FILE_AT=/lightscratch/users/$GASPAR/keys/hf_key.txt \\\n  -e WANDB_API_KEY_FILE_AT=/lightscratch/users/$GASPAR/keys/wandb_key.txt \\\n  -e GITCONFIG_AT=/lightscratch/users/$GASPAR/.gitconfig \\\n  -e GIT_CREDENTIALS_AT=/lightscratch/users/$GASPAR/.git-credentials \\\n  -e VSCODE_CONFIG_AT=/lightscratch/users/$GASPAR/.vscode-server \\\n  --backoff-limit 0 \\\n  --run-as-gid 84257 \\\n  --node-pool h100 \\\n  --gpu 1 \\\n  -- sleep infinity\n</code></pre> <p>Note: If you have issue with the job not being launched (after doing a <code>describe</code>), ensure that there is such an image in the registry. You can build your image following the docker tutorial. Note: It is heavily recommended to save this command into a <code>shell</code> file, to easily edit it and launch jobs with <code>bash connect.sh</code> for instance. You may have two files, one for CPU-only jobs and one for GPU (make sure to give them different names). Generally you don't need more than those two. Important to know: jobs with GPUs are time limited, they are automatically shut down after 2 hours of using no GPUs, whereas there is no such limitation for CPU-only jobs.</p> <p>Explanation:</p> <ul> <li><code>name</code> is the name of the job</li> <li><code>image</code> is the link to the docker image that will be attached to the cluster. Please note that you may need to change the image path if you pushed your image on another link. See Building Docker image for the RCP</li> <li><code>pvc</code> determines which scratch will be mounted to the job. The argument is of the form: <code>name_of_the_scratch:/mount/path/to/scratch</code>. Here the we are mounting the scratch named <code>light-scratch</code> to the local path <code>/lightscratch</code> This part may cause an error because of the LIGHT migration </li> <li><code>gpu</code> is the number of GPUs that you want to claim for this job (larger amount of GPU will be harder to get, as ressources are limited)</li> </ul> <p>We can check the outputs of our container and the status of the job using the following commands respectively. <pre><code># Your terminal\n\nrunai logs base-job\nrunai describe job base-job\n</code></pre></p> <p>To end a job, run the command: <pre><code># Your terminal\n\nrunai delete job base-job\n</code></pre></p> <p>You can access your job by doing <pre><code># Your terminal\n\nrunai bash base-job\n</code></pre></p> <p>You should see a terminal opening. By default it redirects you to the folder <code>/workspace</code>. This folder is not persistent, it is heavily recommended to always start using the job by moving to your personal folder you made earlier, at <code>/lightscratch/users/$GASPAR_USER</code>.</p> <p>You may enter the following command in your new terminal to ensure that you have indeed a GPU:  <pre><code># Job terminal\n\nnvidia-smi\n</code></pre></p> <p>Once you are done, run the following command to delete the job: <pre><code># Your terminal\n\nrunai delete job base-job\n</code></pre></p>"},{"location":"clusters/rcp/rcp/#6-vscode-connection","title":"6. VSCode connection","text":""},{"location":"clusters/rcp/rcp/#mac-and-linux","title":"Mac and Linux","text":"<p>Once we have the container running on a node of the RCP cluster, we can attach to it in VSCode. To do this, we need to have the following extensions installed:</p> <ul> <li>Kubernetes</li> <li>Dev containers</li> </ul> <p>From the Kubernetes menu, we can see the IC and the RCP Cluster. We will enter the menu of the RCP Cluster -&gt; Workloads -&gt; Pods and we will see our container with a green indicator showing that it is running. Right-clicking on it will give us the option to \"Attach to Visual Studio\". Upon clicking, the editor will open in a new window within the container. We are then invited to open a folder, it should be our personal folder (<code>/lightscratch/users/$GASPAR</code>) by default, select it. When opening a new terminal, we should find ourselves directly in our personal folder, if needed we can move there with <code>cd</code> in the terminal. We can install new extensions on VS code, and they will be saved for future sessions.</p>"},{"location":"clusters/rcp/rcp/#windows-wsl-connection","title":"Windows (WSL connection)","text":"<p>For WSL setup, you will need kubernetes on your Windows host because VSCode is going to look for it on the host (and not in WSL).</p> <p>In Windows terminal (not WSL), run:</p> <pre><code># Windows terminal\n\ncurl.exe -LO \"https://dl.k8s.io/release/v1.32.0/bin/windows/amd64/kubectl.exe\"\n</code></pre> <p>Create a folder <code>~/.kube</code> in Windows: <pre><code># Windows terminal\n\nmkdir ~/.kube/\n</code></pre></p> <p>In WSL, claim a job and copy the kube configuration file from WSL to Windows <pre><code># WSL terminal\n\nrunai submit \\\n  --name base-job \\\n  --image registry.rcp.epfl.ch/multimeditron/basic:latest-$GASPAR\\\n  --pvc light-scratch:/lightscratch \\\n  --large-shm \\\n  -e NAS_HOME=/lightscratch/users/$GASPAR \\\n  -e HF_API_KEY_FILE_AT=/lightscratch/users/$GASPAR/keys/hf_key.txt \\\n  -e WANDB_API_KEY_FILE_AT=/lightscratch/users/$GASPAR/keys/wandb_key.txt \\\n  -e GITCONFIG_AT=/lightscratch/users/$GASPAR/.gitconfig \\\n  -e GIT_CREDENTIALS_AT=/lightscratch/users/$GASPAR/.git-credentials \\\n  -e VSCODE_CONFIG_AT=/lightscratch/users/$GASPAR/.vscode-server \\\n  --backoff-limit 0 \\\n  --run-as-gid 84257 \\\n  --node-pool h100 \\\n  --gpu 1 \\\n  -- sleep infinity\n\ncp ~/.kube/config /mnt/c/Users/$WINDOWS_USERNAME/.kube/config\n</code></pre></p> <p>Open VSCode. Install this extension: https://marketplace.visualstudio.com/items?itemName=mtsmfm.vscode-k8s-quick-attach. </p> <p>To attach VSCode to your job: Go to View -&gt; Command Palette (or Ctrl+Shift+P), search for \"k8s quick attach: Quick attach k8s Pod\" -&gt; rcp-caas -&gt; runai-mlo-GASPAR -&gt; meditron-basic-0-0 -&gt; /lightscratch/users/$GASPAR_USER.</p>"},{"location":"clusters/rcp/rcp/#vscode-troubleshooting","title":"VSCode Troubleshooting","text":"<p>If you encounter the following error: </p> <p>Run: <pre><code># WSL terminal\n\nrunai login\ncp ~/.kube/config /mnt/c/Users/$WINDOWS_USERNAME/.kube/config\n</code></pre> And try to attach VSCode again</p>"},{"location":"clusters/rcp/rcp/#more-ressources","title":"More ressources","text":"<ul> <li>EPFL RCP Wiki</li> <li>runai submit Documentation</li> </ul>"},{"location":"clusters/rcp/rcp_docker/","title":"Building Docker image for the RCP","text":"<p>To build a Docker image for the RCP, we will use the LiGHT cluster template.</p> <p>Prerequisites: - You need docker on your computer - A Linux environment (as we are building docker images that target Linux) - Good specs, building docker images require lots of resources in RAM and can take lots of space in your disk. Make sure that your computer is \"good enough\"</p>"},{"location":"clusters/rcp/rcp_docker/#setup","title":"Setup","text":"<p>On your machine:</p> <pre><code>git clone https://github.com/EPFLiGHT/LiGHT-cluster-template.git\n</code></pre> <p>The repository contains tutorials and reproducibility scripts for many clusters, but we will only focus on the RCP in thus tutorial.</p> <pre><code>cd LiGHT-cluster-template/installation/docker-amd64-cuda\n</code></pre> <p>We will build the docker images in the following way:</p> <ol> <li>Build a generic docker image that contains all the dependencies (NVidia drivers, pip, uv, git, etc.)</li> <li>Build many user docker images extending this generic image with the right permisions</li> </ol> <p>This folder contains a <code>template.sh</code> file which contains all the available functions</p>"},{"location":"clusters/rcp/rcp_docker/#creating-a-project-on-the-epfl-registry","title":"Creating a project on the EPFL registry","text":"<p>EPFL has a registry of docker images which works in a similar way to the Docker hub. This registry is used to store docker images and is only accessible through the EPFL network (you may use the EPFL VPN).</p> <p>Connect to registry.rcp.epfl.ch with the VPN. To push new docker images, you need to create a project, by clicking on \"New project\". </p> <p>IMPORTANT: Beware that you will have to change the link of the docker images in the scripts according to the actual path of your project (mainly replacing multimeditron/basic by the right path).</p>"},{"location":"clusters/rcp/rcp_docker/#generating-the-env-file","title":"Generating the .env file","text":"<p>Run the following command:</p> <pre><code>./template.sh env\n</code></pre> <p>This command create a (hidden) <code>.env</code> file which looks like this:</p> <pre><code># All user-specific configurations are here.\n\n## For building:\n# Which docker and compose binary to use\n# docker and docker compose in general or podman and podman-compose for CSCS Clariden\nDOCKER=docker\nCOMPOSE=\"docker compose\"\n# Use the same USRID and GRPID as on the storage you will be mounting.\n# USR is used in the image name and must be lowercase.\n# It's fine if your username is not lowercase, jut make it lowercase.\nUSR=john\nUSRID=1000\nGRPID=984\nGRP=users\n# PASSWD is not secret,\n# it is only there to avoid running password-less sudo commands accidentally.\nPASSWD=john\n# LAB_NAME will be the first component in the image path.\n# It must be lowercase.\nLAB_NAME=users\n\n#### For running locally\n# You can find the acceleration options in the compose.yaml file\n# by looking at the services with names dev-local-ACCELERATION.\nPROJECT_ROOT_AT=/path/to/LiGHT-cluster-template\nACCELERATION=cuda\nWANDB_API_KEY=\n# PyCharm-related. Fill after installing the IDE manually the first time.\nPYCHARM_IDE_AT=\n\n\n####################\n# Project-specific environment variables.\n## Used to avoid writing paths multiple times and creating inconsistencies.\n## You should not need to change anything below this line.\nPROJECT_NAME=template-project-name\nPACKAGE_NAME=template_package_name\nIMAGE_NAME=${LAB_NAME}/${USR}/${PROJECT_NAME}\nIMAGE_PLATFORM=amd64-cuda\n# The image name includes the USR to separate the images in an image registry.\n# Its tag includes the platform for registries that don't hand multi-platform images for the same tag.\n# You can also add a suffix to the platform e.g. -jax or -pytorch if you use different images for different environments/models etc.\n</code></pre> <p>Edit this file by setting <code>LAB_NAME</code> to the name of the project you have given in the previous step, and USR and PASSWD accordingly. Set the <code>PROJECT_NAME</code> to a meaningful name to group the docker images that belong to the same project. Here we will choose to set it up to <code>basic</code>. Docker images with the same <code>PROJECT_NAME</code> will appear together in the <code>registry.rcp.epfl.ch</code> interface.</p> <p>We also change the <code>IMAGE_NAME</code> to another template for organization purpose.</p> <p>Edit the corresponding lines to the following lines:</p> <pre><code>LAB_NAME=multimeditron # Replace it with the name of your project\nPROJECT_NAME=basic # Optional: Change this to a more meaningful name (e.g. vllm or axolotl for instance)\nIMAGE_NAME=${LAB_NAME}/${PROJECT_NAME}\n</code></pre>"},{"location":"clusters/rcp/rcp_docker/#building-the-generic-docker-image","title":"Building the generic docker image","text":"<p>Beware that this docker image uses the <code>Dockerfile</code> to build. You may need to change the base Docker image in <code>compose-base.yaml</code>:</p> <pre><code>BASE_IMAGE: nvcr.io/nvidia/pytorch:pytorch:24-07-py3 # (Optional: Change this tag to a newer version)\n</code></pre> <p>You can change this line to another newer version that you can find here. This will change the version of the NVidia driver that many libraries rely on. Some of the latest features may only be compatible with the newer version of the drivers (for instance the flash-attn library).</p> <p>To build the generic docker image run the following command:</p> <pre><code>./template.sh build_generic\n</code></pre> <p>Useful commands:</p> <p><code>docker ps</code>: To list all the docker images on your machine. Search for the one that you just built, it should have the name that you set up in <code>IMAGE_NAME</code>. So if we are using multimeditron, it will be tagged as <code>multimeditron/basic:amd64-cuda-root-latest</code> and <code>multimeditron/basic:amd64-cuda-root-&lt;some git commit hash&gt;</code></p> <p><code>docker run --rm -it --entrypoint bash multimeditron/basic:amd64-cuda-root-latest</code>: To bash into your new docker image and test if you have everything installed correctly.</p>"},{"location":"clusters/rcp/rcp_docker/#building-the-user-docker-images","title":"Building the user docker images","text":"<p>Find your user id:</p> <pre><code>ssh $GASPAR@haas001.rcp.epfl.ch\n</code></pre> <p>Then use your EPFL password to log in, then find your UID by checking at the beginning of the output of this command:</p> <pre><code>id $GASPAR\n</code></pre> <p>Connect to groups.epfl.ch, \"Groups I'm member of\" and search for light-scratch. You need the <code>GID</code> field.</p> <p>To build user docker images, you need to edit the <code>.env</code> file. You need to change the following attributes:</p> <pre><code>GRPID=&lt;GID of the light-scratch&gt;\n\nUSRID=&lt;UID of the user&gt;\nUSR=&lt;Username of the user, the GASPAR&gt;\nPASSWD=&lt;Username of the user&gt;\n</code></pre> <p>To build the user docker image, run:</p> <pre><code>./template.sh build_user\n</code></pre>"},{"location":"clusters/rcp/rcp_docker/#push-the-docker-images","title":"Push the docker images","text":"<p>Login to the registry:</p> <pre><code>docker login\n</code></pre> <p>The login informations are your EPFL credentials</p> <pre><code>./template.sh push_generic RCP\n./template.sh push_user RCP\n</code></pre>"}]}